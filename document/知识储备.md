# 知识储备

### 传统Transformer模型结构

**`Input Embedding` + `Positional Encoding`** -> **`Encoder`** -> **`Decoder`** -> **`线性层/Softmax`**

Encoder和Decoder，又有多个Encoder layer和Decoder layer，里面又有Multi-head-Attention和FNN，带掩码的多头自注意力机制（(回顾自己已经生成的部分)）、交叉注意力机制（ (查询编码器的输出，理解原文)）以及FNN。

**FNN（Feed-Forward Netrwork）**：标准的神经网络，它对自注意力层输出的每个词的表示进行一次独立的、非线性的变换，进一步提取特征。**MoE（混合专家）模型**中，就是将这个巨大的FFN层，替换成了一个路由器和多个小型的“专家”FFN。

#### 现有的主流LLM（Decoder-only）

```
Input (Prompt + 已生成文本)` -> `Input Embedding + Positional Encoding` -> `Decoder` -> `线性层/Softmax
```

**`Decoder`**，由多个 `Decoder layer` 堆叠而成，里面主要有：**`带掩码的多头自注意力机制`**（统一理解Prompt和已生成内容的全局上下文，是模型中**唯一的**注意力机制）和 **`FNN`**（对每个词的表示进行独立的非线性变换，深入提取特征）。

​	**编码器 (Encoder) 没了**：整个负责“深度理解原文”的独立部门被裁撤了。

​	**交叉注意力机制 (Cross-Attention) 没了**：因为编码器这个“信息源”不存在了，所以解码器内部那个负责“查询原文理解”的交叉注意力机制自然也就没有存在的必要了。

​	**带掩码的自注意力机制 (Masked Self-Attention) 承担了全部职责**：这个模块现在是模型的核心。它既要负责**“向后看”**，深度理解用户输入的Prompt以及自己已经生成的所有内容，又要负责**“向前看”**（预测下一个词），统一了理解和生成的双重任务。

### **什么是MoE？**

传统的LLM是将所有的知识都塞到整个参数里面，这就导致每次回答问题都要调动全部的参数。

MoE采用分而治之的思想，在处理任何一个token时，实际被激活参与计算的**“有效参数量”**非常小（只有被选中的K个专家的参数）

分为 Experts 专家网络和 Gating Router 门控网络，

**工作流程**

一个输入token进入MoE层。

**路由器（Gating Network）**对这个token进行分析，并输出一个权重列表，决定将这个任务分配给哪些专家，以及每个专家的权重是多少。

通常系统会选择**Top-K**（K通常很小，比如1或2）个得分最高的专家。

这个token的数据**只被发送给这K个被选中的专家**进行计算。

**所有其他的专家网络在这次计算中保持“沉默”，完全不消耗计算资源。**

K个专家的输出结果，根据路由器给出的权重进行加权求和，形成最终的输出。

### 并行策略

##### **DP – Data Parallelism 数据并行**

- 每个设备（GPU/NPU）都放一份完整模型，切分 **batch 数据**。
- 每个 rank 只算自己这部分样本的 loss 和梯度。
- 通过 **all-reduce** 把梯度同步，然后更新模型参数。
  👉 优点：实现简单，扩展性好。缺点：显存需求随模型大小增长。

------

##### **TP – Tensor Parallelism 张量并行**

- 把 **单层里的矩阵乘法（比如 WQ/WK/WV, FFN）** 按列/行切分到不同设备计算。解决单个算子太大的问题。
- 每个设备持有一部分权重，前向/反向过程中需要 all-reduce 聚合。
- 相比于PP，TP通信更频繁，因此由NVLink，而PP是按层切分的，PP只要保证层连接处通信即可。
- 👉 优点：可以训练超大模型（参数切开了）。缺点：通信频繁，带宽压力大。

------

##### **PP – Pipeline Parallelism 流水线并行**

- 把模型的 **层（layer）** 切分成几个 stage，每个 stage 放在不同设备。解决模型整体太深的问题
- 前向时像工厂流水线：样本在 stage1 → stage2 → stage3 传递。
- 为避免空转，常用 **micro-batch** 切分，保证流水线“满负荷”。
  👉 优点：突破显存瓶颈，减少单卡模型大小。缺点：调度复杂，有 pipeline bubble。

##### Micro_batch

假设有两张卡，DP=1，TP=1，PP=2 ，那么卡0在工作的时候，卡1闲置；卡1工作的时候，卡0限制。

这个叫做”流水线气泡“。为了解决气泡问题，提出了Micro_batch。把一大批数据分成4小份，把一大批数据切成 4 小份（Micro-batch 1, 2, 3, 4）。**卡0** 处理完 MB1，立刻扔给 **卡1**。**卡1** 开始处理 MB1，同时 **卡0** 紧接着处理 MB2。

Micro Batch 的数量（Num）应该是 PP（Stage数）的 4 倍以上。如果能做到 **8 倍** 甚至更多，那就更完美，但由于显存限制，通常很难无限增加。

------

##### **CP – Context Parallelism 上下文并行**

- 针对 **自注意力 (Self-Attention)** 的 KV 矩阵，把序列长度维度切分。
- 每个设备只存部分 token 的 KV，计算时需要跨设备通信。
  👉 优点：缓解长序列注意力显存瓶颈。

------

##### **EP – Expert Parallelism 专家并行（MoE 专用）**

- 在 Mixture-of-Experts (MoE) 模型里，不同专家（FFN 模块）放在不同设备。
- 路由器 (router) 把 token 分配到对应专家上计算。
  👉 优点：参数量巨大但计算量可控。缺点：负载均衡和通信复杂。

------

##### 2. ZeRO (Zero Redundancy Optimizer)

这是 DeepSpeed 提出的显存优化方法，把 **优化器状态 / 梯度 / 模型参数** 在设备间切分，而不是每个 rank 存一份：

- **Stage 1**：优化器状态分片（optimizer states sharding）
- **Stage 2**：再加上梯度分片（gradient sharding）
- **Stage 3**：再加上参数分片（parameter sharding）
- **Stage 3+**：加上 offload（显存不足时把部分内容放 CPU/NVMe）

👉 本质：用 **通信换显存**，每张卡只存自己负责的部分，训练时通过聚合/广播来完成计算。

------

##### 3. Activation Recomputation (激活重计算)

- 正常训练时需要保存 **每一层的中间激活值**（forward 结果），反向时才能用。
- 但激活占显存非常大（尤其长序列）。
- **激活重计算**：只保存一部分关键节点，其余激活在反向传播时 **重新计算一次 forward** 得到。
  👉 牺牲 **算力（多一次 forward 计算）**，换取 **显存降低**。

------

##### 4. Offloading

- 当显存（GPU/NPU memory）不够时，把一些数据转移到 **CPU 内存** 或 **NVMe SSD**。
- 常见 offload 对象：
  - **优化器状态**（Adam 的 m/v 向量很大）
  - **参数**
  - **激活值**
- 框架如 DeepSpeed ZeRO-Offload 就支持这种方式。
  👉 用 **带宽换显存**，训练速度下降，但能跑更大模型。

### 什么是DeepSeppd？

是微软开源的一个大规模深度学习训练优化框架。

核心功能：

1. ZeRO优化器(Zero Redundancy Optimizer)，用分片的方式吧优化器的状态/梯度/参数拆分到不同的GPU上，避免重复存储。

2. 并行训练支持。支持DP TP PP，并且可以跟ZeRO结合。还支持MoEt的EP。还可以跟Megatron-LM集成。

3. 显存优化。

   ​	**Activation Checkpointing (激活重计算)**：减少保存的中间结果，用算力换显存。

   ​	**Offloading**：把优化器/参数搬到 CPU 或 NVMe。

   ​	**量化训练 (FP16, BF16, 8bit optimizer)**。

## AReal

rollout和train完全解耦，rollout worker一直生成样本，traine worker只要收集到足够的样本就进行训练。

## Parital Rollout

启动更多的rollout请求，当完成部分请求之后就进行训练，剩余的rollout请求异步完成或者缓存。



## Megetron

出自 **NVIDIA**，是一个 **大规模 Transformer 训练框架**，关键技术有TP PP SP/CP 融合算子。

常常和DeepSpeed结合使用

## DeepSpeed

出自 **微软 (Microsoft)**，是一个 **大规模分布式训练和推理优化库**。关键技术有ZeRO（把优化器状态 / 梯度 / 参数分片存放到不同 GPU，极大节省显存）

常常和Megetron结合使用

## SGLang

用来运行LLM并控制其输出（输出格式等）的编程语言和引擎



**LLaMA** 系列使用 **SentencePiece** Tokenizer，其文件通常是 `tokenizer.model`。

**Qwen2.5** 系列使用基于 **BPE** 的 Tokenizer，其文件通常是 `vocab.json`, `merges.txt`, 和 `tokenizer.json`





## Infra 知识点

为什么一个patch size的大小等于infer_dp *chunk_size *num_rollouts？

dp就是data parallel，就是有多少个同时进行的推理副本，

chunk_size就是每个副本一次给多少个prompt

在GRPO中，num_rollouts就是针对一个prompt生成多少个结果。这也是GRPO和PPO的区别，PPO需要依赖一个额外的Critic Model来预测一个Baseline分数；而GRPO则是改用”组内平均值“作为Baseline分数，组内多少个回答就由num_rollouts决定









## 现有框架总结

AReal

VeRL

slime

### ROLL

![image-20250914151806223](F:\Desktop\进度安排.assets\image-20250914151806223.png)

一个中央控制器来统一协调管理。将底层的并行计算任务抽象成统一的并行工作单元（parallel worker）。

ROLL接收用户定义的RL**数据流图（dataflow graph）**及其相关配置作为输入。基于此输入，**分布式执行器与调度器（distributed executor and scheduler）**负责协调各个工作单元和调度器。**自动设备映射（AutoDeviceMapping）**模块管理已配置的资源池**（resource Pool）**中的资源，并高效地将工作单元和调度器绑定到其分配的资源上。

**并行策略 (Parallel Strategy)**。ROLL中的RL训练包含训练、推理和生成三个阶段。我们集成了**MegatronCore**和**DeepSpeed**来加速LLM训练，支持包括DP、PP、TP、CP和EP在内的先进5D并行策略。得益于DeepSpeed，ROLL还支持ZeRO2、ZeRO3和ZeRO-offload。此外，我们提供梯度检查点和卸载策略以显著减少GPU内存消耗，从而能够在资源受限的设备上高效执行。对于推理和生成阶段，我们集成了**vLLM**和**SGLang**，为ROLL配备了TP、EP、PP来加速推理和生成阶段。



**Rollout Scheduler：**The Rollout Scheduler allows users to schedule the lifecycle of each request at the granularity of **individual samples, rather than batches**, during the generation stage. Particularly, the Rollout Scheduler can **dynamically add and abort requests based on current resource availability and the progress of response generation.**

AutoDeviceMapping and Resource Pool：自动设备映射模块协调资源池中的一组CPU和GPU资源，并将它们绑定到工作单元和调度器上。

![image-20250914151819119](F:\Desktop\进度安排.assets\image-20250914151819119.png)





### Rlinf

代码开源但论文未发布。多了M2Flow（(Macro-to-Micro Flow）

把宏观逻辑和微观执行解耦。

宏观逻辑：逻辑工作流的构建，例如

```python
# 1. 生成阶段
responses = actor_model.generate(prompts)

# 2. 推理/打分阶段
rewards = reward_model.score(responses)

# 3. 训练阶段
actor_model.update(responses, rewards)
```

微观逻辑：系统是如何高效完成具体的执行计划的。物理通信调度；数据如何切分，如何打包，通过哪个网络路径发送等。

# 整个框架

RL训练主要包括generation (inference)和training两个阶段，generation需要搞笑的KV cache管理和解码内核；train需要DP TP PP等等。他们需要不同的最优并行化策略。




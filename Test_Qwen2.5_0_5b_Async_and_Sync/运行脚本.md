## 1. 异步命令

```shell
# 加上这个export 不然会提示找不到megatron
export PYTHONPATH=$PYTHONPATH:/home/ma-user/work/Megatron-LM
#!/usr/bin/env bash
set -xuo pipefail

project_name='GRPO-Qwen2.5-0.5b-Base-MATH'
exp_name='GRPO-Qwen2.5-0.5b-Base-MATH-2gpu-async'


# 定义日志文件名，建议带上时间戳防止覆盖
LOG_FILE="/home/ma-user/work/async_train_log_$(date +%Y%m%d_%H%M%S).txt"
echo "Training logs will be saved to: ${LOG_FILE}"


RAY_DATA_HOME=${RAY_DATA_HOME:-"${HOME}/verl"}
MODEL_PATH="/home/ma-user/work/models/Qwen2.5-0.5B-Instruct"
CKPTS_DIR=${CKPTS_DIR:-"${RAY_DATA_HOME}/ckpts/${project_name}/${exp_name}"}
TRAIN_FILE="/home/ma-user/work/data/gsm8k/train.parquet"
TEST_FILE="/home/ma-user/work/data/gsm8k/test.parquet"

rollout_mode="async"
rollout_name="vllm" # sglang or vllm
if [ "$rollout_mode" = "async" ]; then
    export VLLM_USE_V1=1
    return_raw_chat="True"
fi
# Algorithm parameters
adv_estimator=grpo

use_kl_in_reward=False
kl_coef=0.0
use_kl_loss=True
kl_loss_coef=0.001
kl_loss_type=low_var_kl

clip_ratio_low=0.2
clip_ratio_high=0.28

# Response length parameters
max_prompt_length=$((1024 * 2))
max_response_length=$((1024 * 3))
enable_overlong_buffer=True
overlong_buffer_len=$((1024 / 2))
overlong_penalty_factor=1.0

loss_agg_mode="token-mean"

# Algorithm
temperature=1.0
top_p=1.0
top_k=-1 # 0 for HF rollout, -1 for vLLM rollout
val_top_p=0.7

# Performance Related Parameter
use_dynamic_bsz=True
actor_ppo_max_token_len=$(((max_prompt_length + max_response_length)))
infer_ppo_max_token_len=$(((max_prompt_length + max_response_length)))
offload=false
train_ppo_micro_batch_size_per_gpu=32
infer_ppo_micro_batch_size_per_gpu=64

optimizer_offload_fraction=0

COMMON_PP=1
COMMON_VPP=null
COMMON_CP=1
COMMON_TP=1
COMMON_EP=1
COMMON_ETP=1

TRAIN_TP=1
INFER_TP=1

# === 强制使用脚本内定义的配置，忽略外部环境变量干扰 ===
ACTOR_PP=$COMMON_PP
ACTOR_VPP=$COMMON_VPP
ACTOR_CP=$COMMON_CP
ACTOR_TP=$TRAIN_TP
ACTOR_EP=$COMMON_EP
ACTOR_ETP=$COMMON_ETP

ROLLOUT_TP=$INFER_TP

REF_PP=$COMMON_PP
REF_VPP=$COMMON_VPP
REF_CP=$COMMON_CP
REF_TP=$TRAIN_TP
REF_EP=$COMMON_EP
REF_ETP=$COMMON_ETP

CRITIC_PP=$COMMON_PP
CRITIC_VPP=$COMMON_VPP
CRITIC_CP=$COMMON_CP
CRITIC_TP=$TRAIN_TP
CRITIC_EP=$COMMON_EP
CRITIC_ETP=$COMMON_ETP

RM_PP=$COMMON_PP
RM_VPP=$COMMON_VPP
RM_CP=$COMMON_CP
RM_TP=$TRAIN_TP
RM_EP=$COMMON_EP
RM_ETP=$COMMON_ETP
# ====================================================

# install mbridge
# pip3 install git+https://github.com/ISEEKYAN/mbridge
USE_MBRIDGE=False
USE_DIST_CKPT=False

# Fully async specific parameters
NNODES_ROLLOUT=1
NNODES_TRAIN=1
NGPUS_PER_NODE=1

train_prompt_bsz=0
# 每个gpu推理的rollout数量代码里面写死了 16
gen_prompt_bsz=1
n_resp_per_prompt=8
train_prompt_mini_bsz=64
total_rollout_steps=$(((512*100)))
test_freq=20
staleness_threshold=0.2
trigger_parameter_sync_step=4
require_batches=1
partial_rollout=True

python -m recipe.fully_async_policy.fully_async_main \
    --config-path=config \
    --config-name='fully_async_ppo_megatron_trainer.yaml'\
    data.train_files="${TRAIN_FILE}" \
    data.val_files="${TEST_FILE}" \
    data.prompt_key=prompt \
    data.truncation='left' \
    data.max_prompt_length=${max_prompt_length} \
    data.max_response_length=${max_response_length} \
    data.train_batch_size=${train_prompt_bsz} \
    data.return_raw_chat=${return_raw_chat} \
    actor_rollout_ref.rollout.n=${n_resp_per_prompt} \
    algorithm.adv_estimator=${adv_estimator} \
    algorithm.use_kl_in_reward=${use_kl_in_reward} \
    algorithm.kl_ctrl.kl_coef=${kl_coef} \
    actor_rollout_ref.model.path="${MODEL_PATH}" \
    actor_rollout_ref.actor.use_kl_loss=${use_kl_loss} \
    actor_rollout_ref.actor.kl_loss_coef=${kl_loss_coef} \
    actor_rollout_ref.actor.clip_ratio_low=${clip_ratio_low} \
    actor_rollout_ref.actor.clip_ratio_high=${clip_ratio_high} \
    actor_rollout_ref.actor.clip_ratio_c=10.0 \
    +actor_rollout_ref.model.override_config.model_config.max_position_embeddings=$((max_prompt_length + max_response_length)) \
    actor_rollout_ref.model.use_fused_kernels=False \
    actor_rollout_ref.actor.use_dynamic_bsz=${use_dynamic_bsz} \
    actor_rollout_ref.actor.ppo_mini_batch_size=${train_prompt_mini_bsz} \
    actor_rollout_ref.actor.ppo_micro_batch_size_per_gpu=${train_ppo_micro_batch_size_per_gpu} \
    actor_rollout_ref.actor.ppo_max_token_len_per_gpu=${actor_ppo_max_token_len} \
    actor_rollout_ref.actor.optim.lr=1e-6 \
    actor_rollout_ref.actor.optim.lr_warmup_steps=10 \
    actor_rollout_ref.actor.optim.lr_decay_style='constant' \
    actor_rollout_ref.actor.optim.weight_decay=0.1 \
    actor_rollout_ref.actor.optim.lr_decay_steps=${total_rollout_steps} \
    +actor_rollout_ref.actor.optim.override_optimizer_config.optimizer_offload_fraction=${optimizer_offload_fraction} \
    +actor_rollout_ref.actor.optim.override_optimizer_config.overlap_cpu_optimizer_d2h_h2d=True \
    +actor_rollout_ref.actor.optim.override_optimizer_config.use_precision_aware_optimizer=True \
    +actor_rollout_ref.actor.optim.override_optimizer_config.optimizer_cpu_offload=True \
    actor_rollout_ref.actor.megatron.use_mbridge=$USE_MBRIDGE \
    actor_rollout_ref.actor.megatron.use_dist_checkpointing=$USE_DIST_CKPT \
    actor_rollout_ref.actor.megatron.param_offload=${offload} \
    actor_rollout_ref.actor.megatron.grad_offload=${offload} \
    actor_rollout_ref.actor.megatron.optimizer_offload=${offload} \
    actor_rollout_ref.actor.megatron.tensor_model_parallel_size=${ACTOR_TP} \
    actor_rollout_ref.actor.megatron.pipeline_model_parallel_size=${ACTOR_PP} \
    actor_rollout_ref.actor.megatron.virtual_pipeline_model_parallel_size=${ACTOR_VPP} \
    actor_rollout_ref.actor.megatron.context_parallel_size=${ACTOR_CP} \
    actor_rollout_ref.actor.megatron.expert_model_parallel_size=${ACTOR_EP} \
    actor_rollout_ref.actor.megatron.expert_tensor_parallel_size=${ACTOR_ETP} \
    +actor_rollout_ref.actor.megatron.override_transformer_config.apply_rope_fusion=True \
    +actor_rollout_ref.actor.megatron.override_transformer_config.masked_softmax_fusion=True \
    +actor_rollout_ref.actor.megatron.override_transformer_config.bias_activation_fusion=True \
    +actor_rollout_ref.actor.megatron.override_transformer_config.bias_dropout_fusion=True \
    +actor_rollout_ref.actor.megatron.override_transformer_config.gradient_accumulation_fusion=True \
    +actor_rollout_ref.actor.megatron.override_transformer_config.deallocate_pipeline_outputs=True \
    +actor_rollout_ref.actor.megatron.override_transformer_config.persist_layer_norm=True \
    +actor_rollout_ref.actor.megatron.override_transformer_config.use_flash_attn=True \
    actor_rollout_ref.actor.entropy_coeff=0 \
    actor_rollout_ref.actor.loss_agg_mode=${loss_agg_mode} \
    actor_rollout_ref.rollout.log_prob_micro_batch_size_per_gpu=${infer_ppo_micro_batch_size_per_gpu} \
    actor_rollout_ref.rollout.log_prob_max_token_len_per_gpu=${infer_ppo_max_token_len} \
    actor_rollout_ref.rollout.gpu_memory_utilization=0.8 \
    actor_rollout_ref.rollout.tensor_model_parallel_size=${INFER_TP} \
    actor_rollout_ref.rollout.enable_chunked_prefill=True \
    actor_rollout_ref.rollout.max_num_batched_tokens=$((max_prompt_length + max_response_length)) \
    actor_rollout_ref.rollout.temperature=${temperature} \
    actor_rollout_ref.rollout.top_p=${top_p} \
    actor_rollout_ref.rollout.top_k=${top_k} \
    actor_rollout_ref.rollout.val_kwargs.temperature=${temperature} \
    actor_rollout_ref.rollout.val_kwargs.top_p=${val_top_p} \
    actor_rollout_ref.rollout.val_kwargs.top_k=${top_k} \
    actor_rollout_ref.rollout.val_kwargs.do_sample=True \
    actor_rollout_ref.rollout.val_kwargs.n=1 \
    actor_rollout_ref.rollout.name=${rollout_name} \
    actor_rollout_ref.rollout.mode=${rollout_mode} \
    actor_rollout_ref.rollout.calculate_log_probs=True \
    actor_rollout_ref.hybrid_engine=False \
    actor_rollout_ref.rollout.enforce_eager=True \
    actor_rollout_ref.rollout.free_cache_engine=True \
    actor_rollout_ref.ref.log_prob_micro_batch_size_per_gpu=${infer_ppo_micro_batch_size_per_gpu} \
    actor_rollout_ref.ref.log_prob_max_token_len_per_gpu=${infer_ppo_max_token_len} \
    actor_rollout_ref.ref.megatron.use_dist_checkpointing=${USE_DIST_CKPT} \
    actor_rollout_ref.ref.megatron.param_offload=${offload} \
    actor_rollout_ref.ref.megatron.tensor_model_parallel_size=${REF_TP} \
    actor_rollout_ref.ref.megatron.pipeline_model_parallel_size=${REF_PP} \
    actor_rollout_ref.ref.megatron.virtual_pipeline_model_parallel_size=${REF_VPP} \
    actor_rollout_ref.ref.megatron.context_parallel_size=${REF_CP} \
    actor_rollout_ref.ref.megatron.expert_model_parallel_size=${REF_EP} \
    actor_rollout_ref.ref.megatron.expert_tensor_parallel_size=${REF_ETP} \
    reward_model.reward_manager=dapo \
    +reward_model.reward_kwargs.overlong_buffer_cfg.enable=${enable_overlong_buffer} \
    +reward_model.reward_kwargs.overlong_buffer_cfg.len=${overlong_buffer_len} \
    +reward_model.reward_kwargs.overlong_buffer_cfg.penalty_factor=${overlong_penalty_factor} \
    +reward_model.reward_kwargs.overlong_buffer_cfg.log=False \
    +reward_model.reward_kwargs.max_resp_len=${max_response_length} \
    trainer.logger=['console','tensorboard'] \
    trainer.project_name="${project_name}" \
    trainer.experiment_name="${exp_name}" \
    trainer.val_before_train=False \
    trainer.save_freq=-1 \
    trainer.total_epochs=1 \
    trainer.resume_mode=auto \
    trainer.log_val_generations=10 \
    trainer.nnodes="${NNODES_TRAIN}" \
    trainer.n_gpus_per_node="${NGPUS_PER_NODE}" \
    trainer.device=npu \
    rollout.nnodes="${NNODES_ROLLOUT}" \
    rollout.n_gpus_per_node="${NGPUS_PER_NODE}" \
    rollout.total_rollout_steps="${total_rollout_steps}" \
    rollout.total_epochs=1 \
    rollout.test_freq="${test_freq}" \
    async_training.staleness_threshold="${staleness_threshold}" \
    async_training.trigger_parameter_sync_step="${trigger_parameter_sync_step}" \
    async_training.require_batches="${require_batches}" \
    async_training.partial_rollout="${partial_rollout}" \
    async_training.use_rollout_log_probs=True $@ 2>&1 | tee ${LOG_FILE}
```

## 2. 同步命令

```shell
set -x

export VLLM_ASCEND_ENABLE_NZ=0
export VLLM_ATTENTION_BACKEND=XFORMERS

# 定义日志文件名，建议带上时间戳防止覆盖
LOG_FILE="/home/ma-user/work/training_log_$(date +%Y%m%d_%H%M%S).txt"
echo "Logs will be saved to: ${LOG_FILE}"

# 使用 2>&1 | tee 将标准输出和错误输出同时写入文件和屏幕
python3 -m verl.trainer.main_ppo \
    algorithm.adv_estimator=grpo \
    data.train_files=/home/ma-user/work/data/gsm8k/train.parquet \
    data.val_files=/home/ma-user/work/data/gsm8k/test.parquet \
    data.train_batch_size=128 \
    data.max_prompt_length=2048 \
    data.max_response_length=3072 \
    data.filter_overlong_prompts=True \
    data.truncation='error' \
    actor_rollout_ref.model.path=/home/ma-user/work/models/Qwen2.5-0.5B-Instruct \
    actor_rollout_ref.actor.optim.lr=5e-7 \
    actor_rollout_ref.model.use_remove_padding=False \
    actor_rollout_ref.actor.entropy_coeff=0.001 \
    actor_rollout_ref.actor.ppo_mini_batch_size=64 \
    actor_rollout_ref.actor.ppo_micro_batch_size_per_gpu=2 \
    actor_rollout_ref.actor.use_kl_loss=True \
    actor_rollout_ref.actor.kl_loss_coef=0.001 \
    actor_rollout_ref.actor.kl_loss_type=low_var_kl \
    actor_rollout_ref.model.enable_gradient_checkpointing=True \
    actor_rollout_ref.actor.fsdp_config.param_offload=False \
    actor_rollout_ref.actor.fsdp_config.optimizer_offload=False \
    actor_rollout_ref.rollout.log_prob_micro_batch_size_per_gpu=2 \
    actor_rollout_ref.rollout.enable_chunked_prefill=False \
    actor_rollout_ref.rollout.tensor_model_parallel_size=2 \
    actor_rollout_ref.rollout.n=8 \
    actor_rollout_ref.rollout.name=vllm \
    actor_rollout_ref.rollout.gpu_memory_utilization=0.7 \
    actor_rollout_ref.rollout.n=8 \
    actor_rollout_ref.ref.log_prob_micro_batch_size_per_gpu=2 \
    actor_rollout_ref.ref.fsdp_config.param_offload=True \
    algorithm.kl_ctrl.kl_coef=0.001 \
    trainer.critic_warmup=0 \
    trainer.logger=console \
    trainer.project_name='verl_grpo_example_gsm8k' \
    trainer.experiment_name='qwen2_7b_function_rm' \
    trainer.n_gpus_per_node=2 \
    trainer.nnodes=1 \
    trainer.save_freq=-1 \
    trainer.test_freq=5 \
    trainer.total_epochs=1 \
    trainer.device=npu $@ 2>&1 | tee ${LOG_FILE}
```

## 3. 结果分析of Gemini

````shell
基于您提供的两份日志（同步 `training_log_...` 和 异步 `async_train_log_...`）以及对应的启动脚本，我为您整理了详细的对比分析报告。

### 核心结论摘要

**异步训练 (Async) 在吞吐量（Throughput）上拥有碾压性的优势（约 3-4 倍提升），但面临“数据陈旧（Staleness）”带来的挑战，导致训练过程中频繁触发暂停等待机制。**

---

### 详细对比分析

#### 1. 训练效率与吞吐量 (Efficiency & Throughput)

这是两者差异最明显的维度。

* **同步 (Sync):**
* **吞吐量 (`perf/throughput`):** 稳定在 **360 ~ 370 tokens/s** 左右。
* **单步耗时 (`perf/time_per_step`):** 约 **620 ~ 630 秒**。
* **分析:** 同步模式下，Actor 生成数据（Rollout）和 Critic/Actor 更新（Train）是串行的。GPU 在生成时无法训练，在训练时无法生成，导致严重的计算资源闲置（Idle），因此吞吐量较低。


* **异步 (Async):**
* **吞吐量 (`perf/throughput`):** 飙升至 **1225 ~ 1228 tokens/s**。
* **单步耗时 (`perf/time_per_step`):** 约 **340 ~ 370 秒** (波动较大)。
* **分析:** 异步模式实现了“生成”与“训练”的并行流水线。从日志看，异步的吞吐量是同步的 **3.3倍** 以上。这意味着在相同的物理时间内，异步模式可以消耗更多的数据，理论上能更快地收敛。



#### 2. 数据陈旧度与稳定性 (Staleness & Stability)

这是异步训练特有的核心问题，在您的日志中表现得非常明显。

* **同步:** 不存在此问题。每次更新都使用当前策略生成的最新数据（On-policy），数学上最稳健，收敛曲线通常更平滑。
* **异步日志分析:**
* **Staleness 指标:** 日志显示 `fully_async/count/staleness_samples` 经常达到 **307**（接近或达到上限）。
* **触发暂停:** 日志中频繁出现以下警告：
```text
[FullyAsyncRollouter][ShouldPause] due to staleness_samples 307 >= max_required_samples 307
[FullyAsyncRollouter][Processor] Received pause signal, waiting for remaining tasks to return...

```


* **原因解读:** 这说明您的 **Rollout（生成）速度远快于 Trainer（训练）速度**。
* 生成端（vLLM/Rollouter）迅速生产了大量数据堆积在队列中。
* 训练端处理不过来，导致队列中的数据版本落后于当前参数版本（变得“陈旧”）。
* 脚本中设置了 `staleness_threshold=0.2`（陈旧度阈值），为了保证训练质量，系统被迫强制暂停生成，等待训练端追赶进度。





#### 3. 奖励与收敛 (Reward & Convergence)

* **同步:** 步数较多（至 Step 58），Reward 呈现稳步上升趋势。
* **异步:** 步数较少（日志仅到 Step 11 左右）。由于存在 Off-policy（策略偏差）的影响，异步训练初期的 Reward 波动可能会比同步稍大，但在吞吐量的加持下，通常能更快达到相同的 Reward 水平。
* *注意：由于异步日志步数太少，暂时无法断定最终收敛效果是否优于同步。*



#### 4. 资源利用率 (Resource Utilization)

* **MFU (Model Flops Utilization):**
* 同步：`perf/mfu/actor` 约为 **1.2%** (0.012)。
* 异步：`perf/mfu/actor` 约为 **5.0% - 5.6%** (0.056)。
* **结论:** 异步模式极大地压榨了 GPU 算力，利用率提升了 **4倍** 左右。



---

### 针对脚本的优化建议

根据您的脚本配置和日志表现，我有以下发现和建议：

**1. 解决生成与训练速度不匹配问题**
日志显示 Rollout 经常暂停 (`Received pause signal`)，说明生成太快，训练太慢。

* **当前配置:** `train_ppo_micro_batch_size_per_gpu=32`
* **建议:** 尝试 **增加训练端的 Batch Size** 或 **增加训练端的梯度累积步数**，或者如果显存允许，稍微降低生成的并发度，让两者速度更匹配，减少该“暂停-启动”带来的开销。
* 或者，接受这种暂停，因为即便暂停，整体吞吐量依然远高于同步。

**2. 步数与 Epoch 理解**
您脚本中设置：

```bash
total_rollout_steps=$(((512*100)))  # 51200
trainer.total_epochs=1

```

* 这表示整个任务只会跑 1 个 Epoch，消化完 51200 个样本就结束。
* 在异步日志中，`global_step` 代表参数更新的次数。如果发现 Step 数增长很快但 Loss 不降，可能需要检查 `trigger_parameter_sync_step=4` 是否设置得当（即每4次数据收集同步一次参数）。

### 总结

**结果非常符合预期：Verl 的异步模式成功用“策略的一致性（Staleness）”换取了“极高的训练速度（Throughput）”。**

如果您追求在有限时间内获得最好的模型效果，**异步（Async）** 是更好的选择；如果您发现模型训练极其不稳定、发散，或者用于调试算法逻辑，则应切回 **同步（Sync）**。
````






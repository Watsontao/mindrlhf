# çŸ¥è¯†å‚¨å¤‡

### ä¼ ç»ŸTransformeræ¨¡å‹ç»“æ„

**`Input Embedding` + `Positional Encoding`** -> **`Encoder`** -> **`Decoder`** -> **`çº¿æ€§å±‚/Softmax`**

Encoderå’ŒDecoderï¼Œåˆæœ‰å¤šä¸ªEncoder layerå’ŒDecoder layerï¼Œé‡Œé¢åˆæœ‰Multi-head-Attentionå’ŒFNNï¼Œå¸¦æ©ç çš„å¤šå¤´è‡ªæ³¨æ„åŠ›æœºåˆ¶ï¼ˆ(å›é¡¾è‡ªå·±å·²ç»ç”Ÿæˆçš„éƒ¨åˆ†)ï¼‰ã€äº¤å‰æ³¨æ„åŠ›æœºåˆ¶ï¼ˆ (æŸ¥è¯¢ç¼–ç å™¨çš„è¾“å‡ºï¼Œç†è§£åŸæ–‡)ï¼‰ä»¥åŠFNNã€‚

**FNNï¼ˆFeed-Forward Netrworkï¼‰**ï¼šæ ‡å‡†çš„ç¥ç»ç½‘ç»œï¼Œå®ƒå¯¹è‡ªæ³¨æ„åŠ›å±‚è¾“å‡ºçš„æ¯ä¸ªè¯çš„è¡¨ç¤ºè¿›è¡Œä¸€æ¬¡ç‹¬ç«‹çš„ã€éçº¿æ€§çš„å˜æ¢ï¼Œè¿›ä¸€æ­¥æå–ç‰¹å¾ã€‚**MoEï¼ˆæ··åˆä¸“å®¶ï¼‰æ¨¡å‹**ä¸­ï¼Œå°±æ˜¯å°†è¿™ä¸ªå·¨å¤§çš„FFNå±‚ï¼Œæ›¿æ¢æˆäº†ä¸€ä¸ªè·¯ç”±å™¨å’Œå¤šä¸ªå°å‹çš„â€œä¸“å®¶â€FFNã€‚

#### ç°æœ‰çš„ä¸»æµLLMï¼ˆDecoder-onlyï¼‰

```
Input (Prompt + å·²ç”Ÿæˆæ–‡æœ¬)` -> `Input Embedding + Positional Encoding` -> `Decoder` -> `çº¿æ€§å±‚/Softmax
```

**`Decoder`**ï¼Œç”±å¤šä¸ª `Decoder layer` å †å è€Œæˆï¼Œé‡Œé¢ä¸»è¦æœ‰ï¼š**`å¸¦æ©ç çš„å¤šå¤´è‡ªæ³¨æ„åŠ›æœºåˆ¶`**ï¼ˆç»Ÿä¸€ç†è§£Promptå’Œå·²ç”Ÿæˆå†…å®¹çš„å…¨å±€ä¸Šä¸‹æ–‡ï¼Œæ˜¯æ¨¡å‹ä¸­**å”¯ä¸€çš„**æ³¨æ„åŠ›æœºåˆ¶ï¼‰å’Œ **`FNN`**ï¼ˆå¯¹æ¯ä¸ªè¯çš„è¡¨ç¤ºè¿›è¡Œç‹¬ç«‹çš„éçº¿æ€§å˜æ¢ï¼Œæ·±å…¥æå–ç‰¹å¾ï¼‰ã€‚

â€‹	**ç¼–ç å™¨ (Encoder) æ²¡äº†**ï¼šæ•´ä¸ªè´Ÿè´£â€œæ·±åº¦ç†è§£åŸæ–‡â€çš„ç‹¬ç«‹éƒ¨é—¨è¢«è£æ’¤äº†ã€‚

â€‹	**äº¤å‰æ³¨æ„åŠ›æœºåˆ¶ (Cross-Attention) æ²¡äº†**ï¼šå› ä¸ºç¼–ç å™¨è¿™ä¸ªâ€œä¿¡æ¯æºâ€ä¸å­˜åœ¨äº†ï¼Œæ‰€ä»¥è§£ç å™¨å†…éƒ¨é‚£ä¸ªè´Ÿè´£â€œæŸ¥è¯¢åŸæ–‡ç†è§£â€çš„äº¤å‰æ³¨æ„åŠ›æœºåˆ¶è‡ªç„¶ä¹Ÿå°±æ²¡æœ‰å­˜åœ¨çš„å¿…è¦äº†ã€‚

â€‹	**å¸¦æ©ç çš„è‡ªæ³¨æ„åŠ›æœºåˆ¶ (Masked Self-Attention) æ‰¿æ‹…äº†å…¨éƒ¨èŒè´£**ï¼šè¿™ä¸ªæ¨¡å—ç°åœ¨æ˜¯æ¨¡å‹çš„æ ¸å¿ƒã€‚å®ƒæ—¢è¦è´Ÿè´£**â€œå‘åçœ‹â€**ï¼Œæ·±åº¦ç†è§£ç”¨æˆ·è¾“å…¥çš„Promptä»¥åŠè‡ªå·±å·²ç»ç”Ÿæˆçš„æ‰€æœ‰å†…å®¹ï¼Œåˆè¦è´Ÿè´£**â€œå‘å‰çœ‹â€**ï¼ˆé¢„æµ‹ä¸‹ä¸€ä¸ªè¯ï¼‰ï¼Œç»Ÿä¸€äº†ç†è§£å’Œç”Ÿæˆçš„åŒé‡ä»»åŠ¡ã€‚

### **ä»€ä¹ˆæ˜¯MoEï¼Ÿ**

ä¼ ç»Ÿçš„LLMæ˜¯å°†æ‰€æœ‰çš„çŸ¥è¯†éƒ½å¡åˆ°æ•´ä¸ªå‚æ•°é‡Œé¢ï¼Œè¿™å°±å¯¼è‡´æ¯æ¬¡å›ç­”é—®é¢˜éƒ½è¦è°ƒåŠ¨å…¨éƒ¨çš„å‚æ•°ã€‚

MoEé‡‡ç”¨åˆ†è€Œæ²»ä¹‹çš„æ€æƒ³ï¼Œåœ¨å¤„ç†ä»»ä½•ä¸€ä¸ªtokenæ—¶ï¼Œå®é™…è¢«æ¿€æ´»å‚ä¸è®¡ç®—çš„**â€œæœ‰æ•ˆå‚æ•°é‡â€**éå¸¸å°ï¼ˆåªæœ‰è¢«é€‰ä¸­çš„Kä¸ªä¸“å®¶çš„å‚æ•°ï¼‰

åˆ†ä¸º Experts ä¸“å®¶ç½‘ç»œå’Œ Gating Router é—¨æ§ç½‘ç»œï¼Œ

**å·¥ä½œæµç¨‹**

ä¸€ä¸ªè¾“å…¥tokenè¿›å…¥MoEå±‚ã€‚

**è·¯ç”±å™¨ï¼ˆGating Networkï¼‰**å¯¹è¿™ä¸ªtokenè¿›è¡Œåˆ†æï¼Œå¹¶è¾“å‡ºä¸€ä¸ªæƒé‡åˆ—è¡¨ï¼Œå†³å®šå°†è¿™ä¸ªä»»åŠ¡åˆ†é…ç»™å“ªäº›ä¸“å®¶ï¼Œä»¥åŠæ¯ä¸ªä¸“å®¶çš„æƒé‡æ˜¯å¤šå°‘ã€‚

é€šå¸¸ç³»ç»Ÿä¼šé€‰æ‹©**Top-K**ï¼ˆKé€šå¸¸å¾ˆå°ï¼Œæ¯”å¦‚1æˆ–2ï¼‰ä¸ªå¾—åˆ†æœ€é«˜çš„ä¸“å®¶ã€‚

è¿™ä¸ªtokençš„æ•°æ®**åªè¢«å‘é€ç»™è¿™Kä¸ªè¢«é€‰ä¸­çš„ä¸“å®¶**è¿›è¡Œè®¡ç®—ã€‚

**æ‰€æœ‰å…¶ä»–çš„ä¸“å®¶ç½‘ç»œåœ¨è¿™æ¬¡è®¡ç®—ä¸­ä¿æŒâ€œæ²‰é»˜â€ï¼Œå®Œå…¨ä¸æ¶ˆè€—è®¡ç®—èµ„æºã€‚**

Kä¸ªä¸“å®¶çš„è¾“å‡ºç»“æœï¼Œæ ¹æ®è·¯ç”±å™¨ç»™å‡ºçš„æƒé‡è¿›è¡ŒåŠ æƒæ±‚å’Œï¼Œå½¢æˆæœ€ç»ˆçš„è¾“å‡ºã€‚

### å¹¶è¡Œç­–ç•¥

##### **DP â€“ Data Parallelism æ•°æ®å¹¶è¡Œ**

- æ¯ä¸ªè®¾å¤‡ï¼ˆGPU/NPUï¼‰éƒ½æ”¾ä¸€ä»½å®Œæ•´æ¨¡å‹ï¼Œåˆ‡åˆ† **batch æ•°æ®**ã€‚
- æ¯ä¸ª rank åªç®—è‡ªå·±è¿™éƒ¨åˆ†æ ·æœ¬çš„ loss å’Œæ¢¯åº¦ã€‚
- é€šè¿‡ **all-reduce** æŠŠæ¢¯åº¦åŒæ­¥ï¼Œç„¶åæ›´æ–°æ¨¡å‹å‚æ•°ã€‚
   ğŸ‘‰ ä¼˜ç‚¹ï¼šå®ç°ç®€å•ï¼Œæ‰©å±•æ€§å¥½ã€‚ç¼ºç‚¹ï¼šæ˜¾å­˜éœ€æ±‚éšæ¨¡å‹å¤§å°å¢é•¿ã€‚

------

##### **TP â€“ Tensor Parallelism å¼ é‡å¹¶è¡Œ**

- æŠŠ **å•å±‚é‡Œçš„çŸ©é˜µä¹˜æ³•ï¼ˆæ¯”å¦‚ WQ/WK/WV, FFNï¼‰** æŒ‰åˆ—/è¡Œåˆ‡åˆ†åˆ°ä¸åŒè®¾å¤‡è®¡ç®—ã€‚
- æ¯ä¸ªè®¾å¤‡æŒæœ‰ä¸€éƒ¨åˆ†æƒé‡ï¼Œå‰å‘/åå‘è¿‡ç¨‹ä¸­éœ€è¦ all-reduce èšåˆã€‚
   ğŸ‘‰ ä¼˜ç‚¹ï¼šå¯ä»¥è®­ç»ƒè¶…å¤§æ¨¡å‹ï¼ˆå‚æ•°åˆ‡å¼€äº†ï¼‰ã€‚ç¼ºç‚¹ï¼šé€šä¿¡é¢‘ç¹ï¼Œå¸¦å®½å‹åŠ›å¤§ã€‚

------

##### **PP â€“ Pipeline Parallelism æµæ°´çº¿å¹¶è¡Œ**

- æŠŠæ¨¡å‹çš„ **å±‚ï¼ˆlayerï¼‰** åˆ‡åˆ†æˆå‡ ä¸ª stageï¼Œæ¯ä¸ª stage æ”¾åœ¨ä¸åŒè®¾å¤‡ã€‚
- å‰å‘æ—¶åƒå·¥å‚æµæ°´çº¿ï¼šæ ·æœ¬åœ¨ stage1 â†’ stage2 â†’ stage3 ä¼ é€’ã€‚
- ä¸ºé¿å…ç©ºè½¬ï¼Œå¸¸ç”¨ **micro-batch** åˆ‡åˆ†ï¼Œä¿è¯æµæ°´çº¿â€œæ»¡è´Ÿè·â€ã€‚
   ğŸ‘‰ ä¼˜ç‚¹ï¼šçªç ´æ˜¾å­˜ç“¶é¢ˆï¼Œå‡å°‘å•å¡æ¨¡å‹å¤§å°ã€‚ç¼ºç‚¹ï¼šè°ƒåº¦å¤æ‚ï¼Œæœ‰ pipeline bubbleã€‚

------

##### **CP â€“ Context Parallelism ä¸Šä¸‹æ–‡å¹¶è¡Œ**

- é’ˆå¯¹ **è‡ªæ³¨æ„åŠ› (Self-Attention)** çš„ KV çŸ©é˜µï¼ŒæŠŠåºåˆ—é•¿åº¦ç»´åº¦åˆ‡åˆ†ã€‚
- æ¯ä¸ªè®¾å¤‡åªå­˜éƒ¨åˆ† token çš„ KVï¼Œè®¡ç®—æ—¶éœ€è¦è·¨è®¾å¤‡é€šä¿¡ã€‚
   ğŸ‘‰ ä¼˜ç‚¹ï¼šç¼“è§£é•¿åºåˆ—æ³¨æ„åŠ›æ˜¾å­˜ç“¶é¢ˆã€‚

------

##### **EP â€“ Expert Parallelism ä¸“å®¶å¹¶è¡Œï¼ˆMoE ä¸“ç”¨ï¼‰**

- åœ¨ Mixture-of-Experts (MoE) æ¨¡å‹é‡Œï¼Œä¸åŒä¸“å®¶ï¼ˆFFN æ¨¡å—ï¼‰æ”¾åœ¨ä¸åŒè®¾å¤‡ã€‚
- è·¯ç”±å™¨ (router) æŠŠ token åˆ†é…åˆ°å¯¹åº”ä¸“å®¶ä¸Šè®¡ç®—ã€‚
   ğŸ‘‰ ä¼˜ç‚¹ï¼šå‚æ•°é‡å·¨å¤§ä½†è®¡ç®—é‡å¯æ§ã€‚ç¼ºç‚¹ï¼šè´Ÿè½½å‡è¡¡å’Œé€šä¿¡å¤æ‚ã€‚

------

##### 2. ZeRO (Zero Redundancy Optimizer)

è¿™æ˜¯ DeepSpeed æå‡ºçš„æ˜¾å­˜ä¼˜åŒ–æ–¹æ³•ï¼ŒæŠŠ **ä¼˜åŒ–å™¨çŠ¶æ€ / æ¢¯åº¦ / æ¨¡å‹å‚æ•°** åœ¨è®¾å¤‡é—´åˆ‡åˆ†ï¼Œè€Œä¸æ˜¯æ¯ä¸ª rank å­˜ä¸€ä»½ï¼š

- **Stage 1**ï¼šä¼˜åŒ–å™¨çŠ¶æ€åˆ†ç‰‡ï¼ˆoptimizer states shardingï¼‰
- **Stage 2**ï¼šå†åŠ ä¸Šæ¢¯åº¦åˆ†ç‰‡ï¼ˆgradient shardingï¼‰
- **Stage 3**ï¼šå†åŠ ä¸Šå‚æ•°åˆ†ç‰‡ï¼ˆparameter shardingï¼‰
- **Stage 3+**ï¼šåŠ ä¸Š offloadï¼ˆæ˜¾å­˜ä¸è¶³æ—¶æŠŠéƒ¨åˆ†å†…å®¹æ”¾ CPU/NVMeï¼‰

ğŸ‘‰ æœ¬è´¨ï¼šç”¨ **é€šä¿¡æ¢æ˜¾å­˜**ï¼Œæ¯å¼ å¡åªå­˜è‡ªå·±è´Ÿè´£çš„éƒ¨åˆ†ï¼Œè®­ç»ƒæ—¶é€šè¿‡èšåˆ/å¹¿æ’­æ¥å®Œæˆè®¡ç®—ã€‚

------

##### 3. Activation Recomputation (æ¿€æ´»é‡è®¡ç®—)

- æ­£å¸¸è®­ç»ƒæ—¶éœ€è¦ä¿å­˜ **æ¯ä¸€å±‚çš„ä¸­é—´æ¿€æ´»å€¼**ï¼ˆforward ç»“æœï¼‰ï¼Œåå‘æ—¶æ‰èƒ½ç”¨ã€‚
- ä½†æ¿€æ´»å æ˜¾å­˜éå¸¸å¤§ï¼ˆå°¤å…¶é•¿åºåˆ—ï¼‰ã€‚
- **æ¿€æ´»é‡è®¡ç®—**ï¼šåªä¿å­˜ä¸€éƒ¨åˆ†å…³é”®èŠ‚ç‚¹ï¼Œå…¶ä½™æ¿€æ´»åœ¨åå‘ä¼ æ’­æ—¶ **é‡æ–°è®¡ç®—ä¸€æ¬¡ forward** å¾—åˆ°ã€‚
   ğŸ‘‰ ç‰ºç‰² **ç®—åŠ›ï¼ˆå¤šä¸€æ¬¡ forward è®¡ç®—ï¼‰**ï¼Œæ¢å– **æ˜¾å­˜é™ä½**ã€‚

------

##### 4. Offloading

- å½“æ˜¾å­˜ï¼ˆGPU/NPU memoryï¼‰ä¸å¤Ÿæ—¶ï¼ŒæŠŠä¸€äº›æ•°æ®è½¬ç§»åˆ° **CPU å†…å­˜** æˆ– **NVMe SSD**ã€‚
- å¸¸è§ offload å¯¹è±¡ï¼š
  - **ä¼˜åŒ–å™¨çŠ¶æ€**ï¼ˆAdam çš„ m/v å‘é‡å¾ˆå¤§ï¼‰
  - **å‚æ•°**
  - **æ¿€æ´»å€¼**
- æ¡†æ¶å¦‚ DeepSpeed ZeRO-Offload å°±æ”¯æŒè¿™ç§æ–¹å¼ã€‚
   ğŸ‘‰ ç”¨ **å¸¦å®½æ¢æ˜¾å­˜**ï¼Œè®­ç»ƒé€Ÿåº¦ä¸‹é™ï¼Œä½†èƒ½è·‘æ›´å¤§æ¨¡å‹ã€‚

### ä»€ä¹ˆæ˜¯DeepSeppdï¼Ÿ

æ˜¯å¾®è½¯å¼€æºçš„ä¸€ä¸ªå¤§è§„æ¨¡æ·±åº¦å­¦ä¹ è®­ç»ƒä¼˜åŒ–æ¡†æ¶ã€‚

æ ¸å¿ƒåŠŸèƒ½ï¼š

1. ZeROä¼˜åŒ–å™¨(Zero Redundancy Optimizer)ï¼Œç”¨åˆ†ç‰‡çš„æ–¹å¼å§ä¼˜åŒ–å™¨çš„çŠ¶æ€/æ¢¯åº¦/å‚æ•°æ‹†åˆ†åˆ°ä¸åŒçš„GPUä¸Šï¼Œé¿å…é‡å¤å­˜å‚¨ã€‚

2. å¹¶è¡Œè®­ç»ƒæ”¯æŒã€‚æ”¯æŒDP TP PPï¼Œå¹¶ä¸”å¯ä»¥è·ŸZeROç»“åˆã€‚è¿˜æ”¯æŒMoEtçš„EPã€‚è¿˜å¯ä»¥è·ŸMegatron-LMé›†æˆã€‚

3. æ˜¾å­˜ä¼˜åŒ–ã€‚

   â€‹	**Activation Checkpointing (æ¿€æ´»é‡è®¡ç®—)**ï¼šå‡å°‘ä¿å­˜çš„ä¸­é—´ç»“æœï¼Œç”¨ç®—åŠ›æ¢æ˜¾å­˜ã€‚

   â€‹	**Offloading**ï¼šæŠŠä¼˜åŒ–å™¨/å‚æ•°æ¬åˆ° CPU æˆ– NVMeã€‚

   â€‹	**é‡åŒ–è®­ç»ƒ (FP16, BF16, 8bit optimizer)**ã€‚

## AReal

rolloutå’Œtrainå®Œå…¨è§£è€¦ï¼Œrollout workerä¸€ç›´ç”Ÿæˆæ ·æœ¬ï¼Œtraine workeråªè¦æ”¶é›†åˆ°è¶³å¤Ÿçš„æ ·æœ¬å°±è¿›è¡Œè®­ç»ƒã€‚

## Parital Rollout

å¯åŠ¨æ›´å¤šçš„rolloutè¯·æ±‚ï¼Œå½“å®Œæˆéƒ¨åˆ†è¯·æ±‚ä¹‹åå°±è¿›è¡Œè®­ç»ƒï¼Œå‰©ä½™çš„rolloutè¯·æ±‚å¼‚æ­¥å®Œæˆæˆ–è€…ç¼“å­˜ã€‚



## Megetron

å‡ºè‡ª **NVIDIA**ï¼Œæ˜¯ä¸€ä¸ª **å¤§è§„æ¨¡ Transformer è®­ç»ƒæ¡†æ¶**ï¼Œå…³é”®æŠ€æœ¯æœ‰TP PP SP/CP èåˆç®—å­ã€‚

å¸¸å¸¸å’ŒDeepSpeedç»“åˆä½¿ç”¨

## DeepSpeed

å‡ºè‡ª **å¾®è½¯ (Microsoft)**ï¼Œæ˜¯ä¸€ä¸ª **å¤§è§„æ¨¡åˆ†å¸ƒå¼è®­ç»ƒå’Œæ¨ç†ä¼˜åŒ–åº“**ã€‚å…³é”®æŠ€æœ¯æœ‰ZeROï¼ˆæŠŠä¼˜åŒ–å™¨çŠ¶æ€ / æ¢¯åº¦ / å‚æ•°åˆ†ç‰‡å­˜æ”¾åˆ°ä¸åŒ GPUï¼Œæå¤§èŠ‚çœæ˜¾å­˜ï¼‰

å¸¸å¸¸å’ŒMegetronç»“åˆä½¿ç”¨

## SGLang

ç”¨æ¥è¿è¡ŒLLMå¹¶æ§åˆ¶å…¶è¾“å‡ºï¼ˆè¾“å‡ºæ ¼å¼ç­‰ï¼‰çš„ç¼–ç¨‹è¯­è¨€å’Œå¼•æ“



**LLaMA** ç³»åˆ—ä½¿ç”¨ **SentencePiece** Tokenizerï¼Œå…¶æ–‡ä»¶é€šå¸¸æ˜¯ `tokenizer.model`ã€‚

**Qwen2.5** ç³»åˆ—ä½¿ç”¨åŸºäº **BPE** çš„ Tokenizerï¼Œå…¶æ–‡ä»¶é€šå¸¸æ˜¯ `vocab.json`, `merges.txt`, å’Œ `tokenizer.json`



## ç°æœ‰æ¡†æ¶æ€»ç»“

AReal

VeRL

slime

### ROLL

![image-20250914151806223](è¿›åº¦å®‰æ’.assets/image-20250914151806223.png)

ä¸€ä¸ªä¸­å¤®æ§åˆ¶å™¨æ¥ç»Ÿä¸€åè°ƒç®¡ç†ã€‚å°†åº•å±‚çš„å¹¶è¡Œè®¡ç®—ä»»åŠ¡æŠ½è±¡æˆç»Ÿä¸€çš„å¹¶è¡Œå·¥ä½œå•å…ƒï¼ˆparallel workerï¼‰ã€‚

ROLLæ¥æ”¶ç”¨æˆ·å®šä¹‰çš„RL**æ•°æ®æµå›¾ï¼ˆdataflow graphï¼‰**åŠå…¶ç›¸å…³é…ç½®ä½œä¸ºè¾“å…¥ã€‚åŸºäºæ­¤è¾“å…¥ï¼Œ**åˆ†å¸ƒå¼æ‰§è¡Œå™¨ä¸è°ƒåº¦å™¨ï¼ˆdistributed executor and schedulerï¼‰**è´Ÿè´£åè°ƒå„ä¸ªå·¥ä½œå•å…ƒå’Œè°ƒåº¦å™¨ã€‚**è‡ªåŠ¨è®¾å¤‡æ˜ å°„ï¼ˆAutoDeviceMappingï¼‰**æ¨¡å—ç®¡ç†å·²é…ç½®çš„èµ„æºæ± **ï¼ˆresource Poolï¼‰**ä¸­çš„èµ„æºï¼Œå¹¶é«˜æ•ˆåœ°å°†å·¥ä½œå•å…ƒå’Œè°ƒåº¦å™¨ç»‘å®šåˆ°å…¶åˆ†é…çš„èµ„æºä¸Šã€‚

**å¹¶è¡Œç­–ç•¥ (Parallel Strategy)**ã€‚ROLLä¸­çš„RLè®­ç»ƒåŒ…å«è®­ç»ƒã€æ¨ç†å’Œç”Ÿæˆä¸‰ä¸ªé˜¶æ®µã€‚æˆ‘ä»¬é›†æˆäº†**MegatronCore**å’Œ**DeepSpeed**æ¥åŠ é€ŸLLMè®­ç»ƒï¼Œæ”¯æŒåŒ…æ‹¬DPã€PPã€TPã€CPå’ŒEPåœ¨å†…çš„å…ˆè¿›5Då¹¶è¡Œç­–ç•¥ã€‚å¾—ç›ŠäºDeepSpeedï¼ŒROLLè¿˜æ”¯æŒZeRO2ã€ZeRO3å’ŒZeRO-offloadã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬æä¾›æ¢¯åº¦æ£€æŸ¥ç‚¹å’Œå¸è½½ç­–ç•¥ä»¥æ˜¾è‘—å‡å°‘GPUå†…å­˜æ¶ˆè€—ï¼Œä»è€Œèƒ½å¤Ÿåœ¨èµ„æºå—é™çš„è®¾å¤‡ä¸Šé«˜æ•ˆæ‰§è¡Œã€‚å¯¹äºæ¨ç†å’Œç”Ÿæˆé˜¶æ®µï¼Œæˆ‘ä»¬é›†æˆäº†**vLLM**å’Œ**SGLang**ï¼Œä¸ºROLLé…å¤‡äº†TPã€EPã€PPæ¥åŠ é€Ÿæ¨ç†å’Œç”Ÿæˆé˜¶æ®µã€‚



**Rollout Schedulerï¼š**The Rollout Scheduler allows users to schedule the lifecycle of each request at the granularity of **individual samples, rather than batches**, during the generation stage. Particularly, the Rollout Scheduler can **dynamically add and abort requests based on current resource availability and the progress of response generation.**

AutoDeviceMapping and Resource Poolï¼šè‡ªåŠ¨è®¾å¤‡æ˜ å°„æ¨¡å—åè°ƒèµ„æºæ± ä¸­çš„ä¸€ç»„CPUå’ŒGPUèµ„æºï¼Œå¹¶å°†å®ƒä»¬ç»‘å®šåˆ°å·¥ä½œå•å…ƒå’Œè°ƒåº¦å™¨ä¸Šã€‚

![image-20250914151819119](è¿›åº¦å®‰æ’.assets/image-20250914151819119.png)





### Rlinf

ä»£ç å¼€æºä½†è®ºæ–‡æœªå‘å¸ƒã€‚å¤šäº†M2Flowï¼ˆ(Macro-to-Micro Flowï¼‰

æŠŠå®è§‚é€»è¾‘å’Œå¾®è§‚æ‰§è¡Œè§£è€¦ã€‚

å®è§‚é€»è¾‘ï¼šé€»è¾‘å·¥ä½œæµçš„æ„å»ºï¼Œä¾‹å¦‚

```python
# 1. ç”Ÿæˆé˜¶æ®µ
responses = actor_model.generate(prompts)

# 2. æ¨ç†/æ‰“åˆ†é˜¶æ®µ
rewards = reward_model.score(responses)

# 3. è®­ç»ƒé˜¶æ®µ
actor_model.update(responses, rewards)
```

å¾®è§‚é€»è¾‘ï¼šç³»ç»Ÿæ˜¯å¦‚ä½•é«˜æ•ˆå®Œæˆå…·ä½“çš„æ‰§è¡Œè®¡åˆ’çš„ã€‚ç‰©ç†é€šä¿¡è°ƒåº¦ï¼›æ•°æ®å¦‚ä½•åˆ‡åˆ†ï¼Œå¦‚ä½•æ‰“åŒ…ï¼Œé€šè¿‡å“ªä¸ªç½‘ç»œè·¯å¾„å‘é€ç­‰ã€‚

# æ•´ä¸ªæ¡†æ¶

RLè®­ç»ƒä¸»è¦åŒ…æ‹¬generation (inference)å’Œtrainingä¸¤ä¸ªé˜¶æ®µï¼Œgenerationéœ€è¦æç¬‘çš„KV cacheç®¡ç†å’Œè§£ç å†…æ ¸ï¼›trainéœ€è¦DP TP PPç­‰ç­‰ã€‚ä»–ä»¬éœ€è¦ä¸åŒçš„æœ€ä¼˜å¹¶è¡ŒåŒ–ç­–ç•¥ã€‚





# æ—¥å¿—

## ä¸€ã€mindrlhfè¿è¡ŒQwen2.5æ­¥éª¤ï¼ˆæç½®-10.1ï¼‰

## 0. æ•´ä½“æ€è·¯

**ï¼ˆ1ï¼‰å‡†å¤‡èµ„æº**

ä¸‹è½½`mindRLHF`å’Œ`mindFormers`ä¸¤ä¸ªæ ¸å¿ƒä»£ç åº“ï¼›

è·å–é¢„è®­ç»ƒæ¨¡å‹ï¼›

è·å–æ•°æ®é›†ï¼›

é…ç½®ç¯å¢ƒã€‚

**ï¼ˆ2ï¼‰å¤„ç†æ¨¡å‹**

è¿è¡Œ`convert_weight.py`è„šæœ¬ï¼Œå°†pytorchçš„æƒé‡æ–‡ä»¶è½¬æ¢æˆmindsporeæ”¯æŒçš„ã€å•æ–‡ä»¶çš„`.ckpt`æ ¼å¼ï¼›

ç”Ÿæˆåˆ†å¸ƒå¼ç­–ç•¥æ–‡ä»¶ï¼Œ

- ç›®çš„æ˜¯ç”Ÿæˆä¸€ä¸ªæŒ‡å¯¼æ–‡ä»¶ï¼Œå‘Šè¯‰ç¨‹åºå¦‚ä½•å°†ä¸€ä¸ªLLMåˆ†åˆ°2å¼ æˆ–å¤šå¼ å¡ä¸Šï¼›
- è¿è¡Œä¸€ä¸ªçŸ­æ—¶é—´çš„ã€ä¼ªé€ çš„å¾®è°ƒä»»åŠ¡ï¼ˆä½¿ç”¨alpacaæ•°æ®é›†ï¼‰ï¼Œè®©mindsporeå§è®¡ç®—å›¾æ„å»ºå‡ºæ¥ï¼Œå¹¶æ ¹æ®è‡ªå·±è®¾ç½®çš„2å¡æˆ–è€…å¤šå¡å¹¶è¡Œé…ç½®ï¼Œè‡ªåŠ¨ä¿å­˜è¿™ä»½â€œå›¾çº¸â€ï¼ˆç­–ç•¥æ–‡ä»¶ï¼‰ï¼›
- æ‰§è¡Œæƒé‡åˆ‡åˆ†ï¼Œè¿è¡Œ`transform_checkpoint.py`è„šæœ¬ï¼Œä¾æ®ä¸Šä¸€æ­¥ç”Ÿæˆçš„â€œå›¾çº¸â€ï¼Œå°†ç¬¬ä¸€æ­¥å¾—åˆ°çš„é‚£ä¸ªå®Œæ•´çš„`.ckpt`æ–‡ä»¶ï¼Œåˆ‡åˆ†æˆ2ä»½æˆ–è€…å¤šä»½åˆ†å¸ƒå¼æƒé‡æ–‡ä»¶

**ï¼ˆ3ï¼‰å¤„ç†æ•°æ®**

ç›®çš„ï¼šåŸå§‹çš„`.jsonl`æ ¼å¼æ–‡ä»¶è¯»å–æ•ˆç‡ä½ï¼Œéœ€è¦è½¬æ¢ä¸ºmindsporeæ”¯æŒçš„`.mindrecord`æ ¼å¼æ–‡ä»¶ï¼›

è¿è¡Œ`rlhf_data.py`è„šæœ¬ï¼Œå°†`GSM8K`çš„`train.jsonl`æ–‡ä»¶ï¼Œè¿åŒQwen2.5çš„tokenizerä¸€èµ·å¤„ç†ï¼Œç”Ÿæˆæœ€ç»ˆç”¨äºè®­ç»ƒçš„`gsm8k_train.mindrecord`æ–‡ä»¶ã€‚

**ï¼ˆ4ï¼‰ç¼–å†™ä¸æ ¸å¯¹é…ç½®æ–‡ä»¶**

æ¨¡å‹å¹¶è¡Œé…ç½®æ–‡ä»¶

- å®šä¹‰æ¨¡å‹æ¶æ„

- å®šä¹‰2å¡çš„å¹¶è¡Œç­–ç•¥

- å®šä¹‰ç›¸å…³çš„é‡è®¡ç®—ç­‰ä¼˜åŒ–ç­–ç•¥

GRPOç®—æ³•é…ç½®æ–‡ä»¶

- å®šä¹‰GRPOç®—æ³•çš„è¶…å‚æ•°ç­‰ç­‰

**ï¼ˆ5ï¼‰å¯åŠ¨è®­ç»ƒ**

è®¾ç½® `PYTHONPATH`ï¼šè¿è¡Œ `export` å‘½ä»¤ï¼Œå‘Šè¯‰ Python å»å“ªé‡Œæ‰¾ `mindformers` å’Œ `mindrlhf` çš„ä»£ç ã€‚

æ‰§è¡Œ `msrun` å‘½ä»¤ï¼š

- è¿™æ˜¯æœ€ç»ˆçš„è®­ç»ƒå¯åŠ¨å‘½ä»¤ã€‚
- å®ƒä¼šä½¿ç”¨ **2 å¡** (`--worker_num=2`) æ¥è¿è¡Œã€‚
- å®ƒä¼šåŠ è½½**ç¬¬ä¸‰æ­¥**å¤„ç†å¥½çš„ `gsm8k` æ•°æ®é›†ã€‚
- å®ƒä¼šåŠ è½½**ç¬¬äºŒæ­¥**åˆ‡åˆ†å¥½çš„ `2å¡` åˆ†å¸ƒå¼æƒé‡ã€‚
- å®ƒä¼šéµå¾ª**ç¬¬å››æ­¥**é…ç½®å¥½çš„æ‰€æœ‰è®­ç»ƒå‚æ•°ã€‚
- æœ€ç»ˆå¼€å§‹çœŸæ­£çš„ GRPO ç®—æ³•å¾®è°ƒï¼Œå¹¶å°†è®­ç»ƒå¥½çš„æ¨¡å‹ä¿å­˜ä¸‹æ¥ã€‚







### 1. æ¨¡å‹ä»¥åŠæ•°æ®é›†è·å–ä¸é¢„å¤„ç†

ä¸‹é¢æ˜¯æ­£å¼çš„æ­¥éª¤

####  1.1 æ¨¡å‹æƒæ–‡ä»¶å’Œtokenizerè·å–

```shell
mkdir models
cd models
mkdir Qwen2.5
mkdir Qwen2.5_new1 
pip install modelscope
modelscope download --model Qwen/Qwen2.5-7B  --local_dir   /home/ma-user/work/models/Qwen2.5


git clone https://gitee.com/mindspore/mindrlhf.git

git clone https://gitee.com/mindspore/mindformers.git
cd mindformers
bash build.sh

cd mindformers/research/qwen2_5

python convert_weight.py \
--torch_ckpt_dir /home/ma-user/work/models/Qwen2.5 \
--mindspore_ckpt_path /home/ma-user/work/models/qwen2_5_ms.ckpt \
--dtype bf16 \
--config_path /home/ma-user/work/mindformers/research/qwen2_5/finetune_qwen2_5_7b_8k.yaml
```

####  1.2 æ¨¡å‹æƒé‡ç¦»çº¿åˆ‡åˆ†

ä½¿ç”¨å¤šå¡åˆ†å¸ƒå¼è®­ç»ƒæ—¶ï¼Œæ‰‹åŠ¨è¿›è¡Œåˆ†å¸ƒå¼æƒé‡åˆ‡åˆ†ï¼Œå…ˆç”Ÿæˆåˆ‡åˆ†æ–¹æ¡ˆï¼Œåœ¨æ‰§è¡Œåˆ‡åˆ†åŠ¨ä½œã€‚

å…ˆè·å–æ•°æ®æ–‡ä»¶/{path}/alpaca-fastchat4096.mindrecordï¼Œæ­¥éª¤å¦‚ä¸‹ï¼š

**v1ï¼ˆåºŸå¼ƒï¼‰**

```shell
å»ºè®®æ–°å¼€ä¸ªterminal
pip install fastchat>=0.2.13     è¿è¡Œä¸äº†å°±æŠŠ>=0.2.13åˆ äº†

git clone https://github.com/tatsu-lab/stanford_alpaca.git
cd /home/ma-user/work/mindformers/mindformers/tools/dataset_preprocess/llama

cd work
mkdir data
cd mindformers/tools/dataset_preprocess/llama
python alpaca_converter.py \
--data_path /home/ma-user/work/stanford_alpaca/alpaca_data.json \
--output_path /home/ma-user/work/data/alpaca-data-conversation.json


cd mindformers/tools/dataset_preprocess/llama

æ­¤å¤„éœ€è¦åœ¨mindformers/tools/dataset_preprocess/llamaä¸‹æ–°å»ºä¸€ä¸ªqwen_preprocess.py   è§æœ€ä¸Šé¢


python qwen_preprocess.py \
--dataset_type qa \
--input_glob /home/ma-user/work/data/alpaca-data-conversation.json \
--model_file /home/ma-user/work/models/Qwen2.5/tokenizer.json \
--seq_length 4096 \
--output_file /home/ma-user/work/data/alpaca-fastchat4096.mindrecord

```

**v2**

```shell
pip install fastchat
git clone https://github.com/tatsu-lab/stanford_alpaca.git   è·å–alpaca æ•°æ®é›†

cd mindformers/research/qwen2/


# æ‰§è¡Œresearch/qwen2/alpaca_converter.pyï¼Œå°†åŸå§‹æ•°æ®é›†è½¬æ¢ä¸ºæŒ‡å®šæ ¼å¼ã€‚
python alpaca_converter.py \
--data_path /home/ma-user/work/stanford_alpaca/alpaca_data.json \
--output_path /home/ma-user/work/data/alpaca-data-conversation.json 
 
# æ‰§è¡Œresearch/qwen2/qwen2_preprocess.pyæ–‡ä»¶ï¼Œè¿›è¡Œæ•°æ®é¢„å¤„ç†å’ŒMindrecordæ•°æ®ç”Ÿæˆã€‚ 
# æ³¨æ„ éœ€è¦ä¿®æ”¹ä¸€ä¸‹qwen2_preprocessçš„ä»£ç 
# æ‰“å¼€ qwen2_preprocess.py
# /home/ma-user/work/mindformers/research/qwen2/qwen2_preprocess.py
# ä¿®æ”¹å¯¼å…¥è¡Œ
# å°†ç¬¬ 27 è¡Œçš„ç»å¯¹å¯¼å…¥ä¿®æ”¹ä¸ºç›¸å¯¹å¯¼å…¥ã€‚
#
# åŸæ¥çš„ä»£ç :
# from research.qwen2.qwen2_tokenizer import Qwen2Tokenizer
# ä¿®æ”¹ä¸º:
# from qwen2_tokenizer import Qwen2Tokenizer

python qwen2_preprocess.py \
 --dataset_type 'qa' \
 --input_glob /home/ma-user/work/data/alpaca-data-conversation.json \
 --vocab_file /home/ma-user/work/models/Qwen2.5/vocab.json \
 --merges_file /home/ma-user/work/models/Qwen2.5/merges.txt \
 --seq_length 4096 \
 --output_file /home/ma-user/work/data/alpaca-fastchat4096.mindrecord
```



æ¥ç€è¿›è¡Œç”Ÿæˆåˆ‡åˆ†æ–¹æ¡ˆï¼Œåœ¨æ‰§è¡Œåˆ‡åˆ†åŠ¨ä½œã€‚

```shell
å…ˆç”Ÿæˆåˆ‡åˆ†æ–¹æ¡ˆï¼Œåœ¨æ‰§è¡Œåˆ‡åˆ†åŠ¨ä½œ
ç”Ÿæˆåˆ‡åˆ†æ–¹æ¡ˆæ—¶ï¼Œmsrun_launcher.shé»˜è®¤æ˜¯8å¡ï¼Œéœ€è¦æ‰‹åŠ¨ä¿®æ”¹


cd /research/qwen2_5

# ç”±äºç”Ÿæˆåˆ‡åˆ†æ–¹æ¡ˆæ—¶ï¼Œmsrun_launcher.shé»˜è®¤æ˜¯8å¡ï¼Œéœ€è¦æ‰‹åŠ¨ä¿®æ”¹  ä¸‹é¢æ˜¯æ‰‹åŠ¨ä¿®æ”¹çš„æ­¥éª¤
cd /home/ma-user/work/mindformers/research/qwen2_5/
cp finetune_qwen2_5_7b_8k.yaml finetune_qwen2_5_7b_2cards.yaml
# ä¿®æ”¹åçš„è¯¦ç»†ä»£ç è§é™„å½•2


bash ../../scripts/msrun_launcher.sh "run_qwen2_5.py \
--config /home/ma-user/work/mindformers/research/qwen2_5/finetune_qwen2_5_7b_2cards.yaml \
--run_mode finetune \
--train_data /home/ma-user/work/data/alpaca-fastchat4096.mindrecord " 2 PORT output/msrun_log False 2000


æ³¨æ„ï¼šPORT æ˜¯ä¸€ä¸ªå ä½ç¬¦ (Placeholder)ï¼Œæ˜¯æ•™ç¨‹é‡Œæé†’æ‚¨éœ€è¦æ‰‹åŠ¨æ›¿æ¢æˆä¸€ä¸ªå…·ä½“æ•°å­—çš„åœ°æ–¹   éœ€è¦é€‰æ‹©ä¸€ä¸ªå½“å‰æœªè¢«å ç”¨çš„ç«¯å£å·ï¼ˆé€šå¸¸é€‰æ‹© 1024 åˆ° 65535 ä¹‹é—´çš„ä»»æ„æ•°å­—ï¼Œæ¯”å¦‚æ•™ç¨‹é‡Œå¸¸ç”¨çš„ 9887ï¼‰ï¼Œç„¶åç”¨è¿™ä¸ªæ•°å­—æ›¿æ¢æ‰å‘½ä»¤ä¸­çš„ PORTã€‚

# è¿™ä¸ªå‘½ä»¤ä¼šåœ¨/research/qwen2_5/strategyè·¯å¾„ä¸‹ç”Ÿæˆå¹¶è¡Œç­–ç•¥æ–‡ä»¶ï¼Œåœ¨ä¸‹ä¸€æ­¥åˆ‡åˆ†ckptæ—¶ä½œä¸ºdst_strategyçš„å€¼ä¼ å…¥ã€‚
# æŸ¥çœ‹æŠ¥é”™æ—¥å¿—tail -f /home/ma-user/work/mindformers/research/qwen2_5/output/msrun_log/worker_0.log
```

ã€è¿™é‡Œå°‘ä¸€æ­¥ï¼Œå°‘äº†åˆ‡ç‰‡çš„ä»£ç ï¼Œå› ä¸ºä¸Šé¢è¿™ä¸€æ­¥ä¸€ç›´åœ¨æŠ¥é”™ã€‘







#### 1.3 æ•°æ®é›†æ–‡ä»¶è·å–ä¸é¢„å¤„ç†

```shell
å»ºè®®æ–°å¼€ä¸ªterminal
cd work
# ä¸‹è½½gsm8kæ•°æ®é›†
git clone https://github.com/openai/grade-school-math.git

# ä¸‹è½½å®Œæˆåï¼Œéœ€è¦è½¬ä¸ºMindSporeä½¿ç”¨çš„.mindrecordæ–‡ä»¶   é¦–å…ˆè¿›å…¥MindRLHFè·¯å¾„ å¹¶æ‰§è¡Œä»¥ä¸‹è„šæœ¬ï¼š
cd /mindrlhf

pip install jsonlines

# æ³¨æ„ï¼Œè¿™é‡Œéœ€è¦æ·»åŠ ç¯å¢ƒå˜é‡ï¼Œè§3. å¯åŠ¨GRPOè®­ç»ƒè„šæœ¬


python examples/grpo/qwen_grpo_tutorial/rlhf_data.py \
--vocab_path /home/ma-user/work/models/Qwen2.5/vocab.json \
--merges_file_path /home/ma-user/work/models/Qwen2.5/merges.txt \
--file_path /home/ma-user/work/grade-school-math/grade_school_math/data/train.jsonl \
--output_path /home/ma-user/work/data/gsm8k_train.mindrecord \
--dataset_type gsm8k



```

## 2. GRPOç®—æ³•åŠæ¨¡å‹é…ç½®

#### 2.1  è®­ç»ƒ/æ¨ç†æ¨¡å‹é…ç½®

è®­ç»ƒæ¨¡å‹çš„é…ç½®æ–‡ä»¶é»˜è®¤ä¸º`model_configs/qwen_grpo/qwen2_5_7b/finetune_qwen2_5_7b.yaml`ï¼Œå…¶ä¸­ç”¨æˆ·å¯ä»¥æ‰‹åŠ¨é…ç½®è®­ç»ƒæ¨¡å‹çš„å¹¶è¡Œç­–ç•¥ï¼š

```yaml
parallel_config:
    data_parallel: 1
    model_parallel: 4
    pipeline_stage: 2
    use_seq_parallel: True
    micro_batch_num: 2
    micro_batch_interleave_num: 2
# å‚æ•°è¯´æ˜
data_parallel:                æ•°æ®å¹¶è¡Œåˆ‡åˆ†ç»„æ•°
model_parallel:               æ¨¡å‹å¹¶è¡Œ(tensor parallel)åˆ‡åˆ†ç»„æ•°
pipeline_stage:               æµæ°´çº¿å¹¶è¡Œåˆ‡åˆ†ç»„æ•°
use_seq_parallel:             æ˜¯å¦ä½¿ç”¨sequence parallel
micro_batch_num:              æµæ°´çº¿å¹¶è¡Œä¸­çš„micro batch number
micro_batch_interleave_num:   å½“model_parallel>1æ—¶,å¯ä»¥è®¾ç½®ä¸º2ä»¥åŠ é€Ÿè®­ç»ƒ
```

æ¨ç†æ¨¡å‹çš„é…ç½®æ–‡ä»¶é»˜è®¤ä¸º`model_configs/qwen_grpo/qwen2_5_7b/predict_qwen2_5_7b_instruct.yaml`ï¼Œå…¶ä¸­ç”¨æˆ·å¯ä»¥æ‰‹åŠ¨é…ç½®æ¨ç†æ¨¡å‹çš„å¹¶è¡Œç­–ç•¥ï¼š

```yaml
parallel_config:
    data_parallel: 2
    model_parallel: 4
    pipeline_stage: 1
# å‚æ•°è¯´æ˜
data_parallel:                æ•°æ®å¹¶è¡Œåˆ‡åˆ†ç»„æ•°
model_parallel:               æ¨¡å‹å¹¶è¡Œ(tensor parallel)åˆ‡åˆ†ç»„æ•°
pipeline_stage:               æµæ°´çº¿å¹¶è¡Œåˆ‡åˆ†ç»„æ•°, å¿…é¡»ä¸º1. å½“å‰æ¨ç†æ¨¡å‹ä¸æ”¯æŒæµæ°´çº¿å¹¶è¡Œ
```

#### 2.2  GRPOè®­ç»ƒç®—æ³•é…ç½®

GRPOè®­ç»ƒç®—æ³•ç›¸å…³é…ç½®å¯ä»¥åœ¨`examples/grpo/qwen_grpo_tutorial/grpo_config.yaml`å†…è¿›è¡Œä¿®æ”¹ï¼ŒåŒ…æ‹¬ä»¥ä¸‹å‚æ•°ï¼š

```yaml
beta: float = 0.01
num_generations: int = 8
num_rollouts: int = 4
epochs: int = 2
start_lr: float = 5e-7
end_lr: float = 1e-10
chunk_size: int = 2
batch_size: int = 2
sync_ref_model: bool = True
ref_model_sync_steps: int = 50

# å‚æ•°è¯´æ˜
beta:                   åå‘è®­ç»ƒGRPO lossä¸­KLæ•£åº¦çš„æƒé‡
num_generations:        æ¨ç†æ¨¡å‹åœ¨æ¯ä¸€æ­¥ä¸­ä¸ºæ¯ä¸ªé—®é¢˜ç”Ÿæˆå¤šå°‘ä¸ªå›ç­”
num_rollouts:           æ¨ç†æ¨¡å‹åœ¨è®­ç»ƒä¹‹å‰ä¼šåå¤è¿›è¡Œå¤šå°‘è½®
epochs:                 åœ¨æ•°æ®é›†ä¸Šæ€»å…±è®­ç»ƒçš„epochsè½®æ•°
start_lr:               åˆå§‹æ—¶åå‘è®­ç»ƒçš„learning rateæ­¥é•¿
end_lr:                 ç»“æŸæ—¶åå‘è®­ç»ƒçš„learning rateæ­¥é•¿, å¿…é¡»ä¸¥æ ¼å°äºäºstart_lr
chunk_size:             æ¨ç†æ¨¡å‹åœ¨æ¯ä¸€æ­¥ä¸­ä¸ºå¤šå°‘ä¸ªé—®é¢˜ç”Ÿæˆå›ç­”
batch_size:             åå‘è®­ç»ƒçš„batch size
sync_ref_model:         æ˜¯å¦æ¯éš”è‹¥å¹²æ­¥å°†ref modelçš„æƒé‡æ›´æ–°ä¸ºæœ€æ–°çš„è®­ç»ƒæ¨¡å‹æƒé‡
ref_model_sync_steps:   è‹¥sync_ref_model=True, ref modelæƒé‡æ›´æ–°çš„é—´éš”æ­¥æ•°
```

#### 2.3 GRPOæ€§èƒ½ç»Ÿè®¡é…ç½®

GRPOæ€§èƒ½ç»Ÿè®¡ç›¸å…³é…ç½®å¯ä»¥åœ¨`examples/grpo/qwen_grpo_tutorial/grpo_config.yaml`å†…è¿›è¡Œä¿®æ”¹ï¼ŒåŒ…æ‹¬ä»¥ä¸‹å‚æ•°ï¼š

```
performance_stats: bool = False

# å‚æ•°è¯´æ˜
performance_stats:                   æ˜¯å¦åœ¨æ—¥å¿—ä¸­æ‰“å°å„æµç¨‹çš„æ‰§è¡Œæ—¶é—´
```

### 3. å¯åŠ¨GRPOè®­ç»ƒè„šæœ¬

é¦–å…ˆè¿›å…¥MindRLHFè·¯å¾„

```shell
# å•å¼€ä¸ªterminal
cd mindrlhf
```

ä½¿ç”¨ä»¥ä¸‹å‘½ä»¤å°†æœ¬åœ°mindrlhfå’Œmindformersä»£ç åº“å‡åŠ å…¥PYTHONPATHï¼ŒMINDFORMERS_PATè·¯å¾„ä¸­

```shell
MINDRLHF_FILE=/{path}/mindrlhf/
MINDFORMERS_FILE=/{path}/mindformers/

export PYTHONPATH="$MINDRLHF_FILE:$MINDFORMERS_FILE:$PYTHONPATH"
export MINDFORMERS_PATH="$MINDFORMERS_FILE $MINDFORMERS_PATH"
```

### å•æœº8å¡æ‹‰èµ·Qwen2.5-7b

éšåä½¿ç”¨ä»¥ä¸‹å‘½ä»¤æ‹‰èµ·å•æœº8å¡GRPOè®­ç»ƒä»»åŠ¡ï¼Œå¯ä»¥å‚è€ƒrun_grpo.sh

```yaml
msrun --worker_num=8 --local_worker_num=8 \
--master_addr=127.0.0.1 --master_port=9887 \
--join=True --log_dir=./prof_vllm_log \
examples/grpo/qwen_grpo_tutorial/main.py \
--config examples/grpo/qwen_grpo_tutorial/grpo_config.yaml \
--dataset_file /path/to/datasets/gsm8k_train.mindrecord \
--tokenizer_dir /path/to/configs/ \
--actor_checkpoint_path /path/to/weights/qwen2_5_7b/ \
--ref_checkpoint_path /path/to/weights/qwen2_5_7b/ \
--generate_checkpoint_path /path/to/weights/qwen2_5_7b/ \
--verifier_function "qwen_accuracy_reward,format_reward" \
--verifier_weight "1.0,1.0" \
--save_checkpoint_dir "/path/to/ckpt/"


# åœ¨è¿è¡Œæ­¤å‘½ä»¤å‰ï¼Œè¯·ç¡®ä¿æ‚¨å·²ç»è®¾ç½®å¥½äº† PYTHONPATH ç¯å¢ƒå˜é‡
# export PYTHONPATH="/home/ma-user/work/mindrlhf:/home/ma-user/work/mindformers:$PYTHONPATH"

msrun --worker_num=8 --local_worker_num=8 \
--master_addr=127.0.0.1 --master_port=9887 \
--join=True --log_dir=./prof_vllm_log \
examples/grpo/qwen_grpo_tutorial/main.py \
--config examples/grpo/qwen_grpo_tutorial/grpo_config.yaml \
--dataset_file /home/ma-user/work/data/gsm8k_train.mindrecord \
--tokenizer_dir /home/ma-user/work/models/Qwen2.5/ \
--actor_checkpoint_path /home/ma-user/work/weights/qwen2_5_7b_distributed/ \
--ref_checkpoint_path /home/ma-user/work/weights/qwen2_5_7b_distributed/ \
--generate_checkpoint_path /home/ma-user/work/weights/qwen2_5_7b_distributed/ \
--verifier_function "qwen_accuracy_reward,format_reward" \
--verifier_weight "1.0,1.0" \
--save_checkpoint_dir "/home/ma-user/work/output/grpo_qwen_7b_ckpt/"


# å‚æ•°è¯´æ˜
# msrun å‚æ•°
worker_num:                   æ€»å¡æ•°
local_worker_num:             å•æœºçš„å¡æ•°
master_addr:                  ä¸»èŠ‚ç‚¹åœ°å€
master_port:                  ä¸»èŠ‚ç‚¹ç«¯å£
join:                         æ˜¯å¦ç­‰å¾…æ‰€æœ‰workeré€€å‡º
log_dir:                      æ—¥å¿—è·¯å¾„
# main.py å‚æ•°
config:                       grpoçš„é…ç½®æ–‡ä»¶
tokenizer_dir:                æ¨¡å‹å¯¹åº”çš„tokenizeræ–‡ä»¶vocab.jsonå’Œmerges.txtæ‰€åœ¨çš„ç›®å½•
dataset_file:                 è®­ç»ƒæ•°æ®é›†mindrecordæ–‡ä»¶çš„è·¯å¾„
save_checkpoint_dir:          è®­ç»ƒckptçš„ä¿å­˜è·¯å¾„
generate_checkpoint_path:        æ¨ç†æ¨¡å‹(åˆ†å¸ƒå¼)ckptæ–‡ä»¶è·¯å¾„
actor_checkpoint_path:       è®­ç»ƒæ¨¡å‹(åˆ†å¸ƒå¼)ckptæ–‡ä»¶è·¯å¾„
ref_checkpoint_path:          å‚è€ƒæ¨¡å‹(åˆ†å¸ƒå¼)ckptæ–‡ä»¶è·¯å¾„
verifier_function:                 rewardå‡½æ•°
verifier_weight:               rewardçš„æƒé‡ç³»æ•°
tensorboard_dir:              tensorboardè½ç›˜è·¯å¾„ï¼Œä»…åœ¨éœ€è¦ä½¿ç”¨tensorboardè®°å½•æ—¶å¼€å¯
tensorboard_queue_size:       tensorboardç¼“å­˜é˜Ÿåˆ—å¤§å°
```









### 9.30ç»“æœ

**é—®é¢˜1**

mindspore2.2.0ç‰ˆæœ¬åœ¨è¿è¡Œconvert_weight.pyæŠ¥é”™ï¼ŒåŸå› æ˜¯å› ä¸ºmindsporeç‰ˆæœ¬å¤ªä½ï¼Œå› æ­¤

```shell
pip install --upgrate minspore
```

æ›´æ–°åˆ°äº†2.7.0ä¹‹åå¯ä»¥æ­£å¸¸è¿è¡Œè¿›è¡Œæƒé‡è½¬æ¢äº†ï¼ˆè½¬æ¢æˆMindSporeä½¿ç”¨çš„.ckptæ–‡ä»¶ï¼‰

**é—®é¢˜2**

```
Traceback (most recent call last):
  File "/home/ma-user/work/mindformers/convert_weight.py", line 101, in <module>
    convert_func(merged_args)
  File "/home/ma-user/work/mindformers/research/qwen2_5/convert_weight.py", line 316, in convert_weight
    convert_pt_to_ms(input_path=para.torch_ckpt_dir, output_path=para.mindspore_ckpt_path, dtype=dtype)
  File "/home/ma-user/work/mindformers/research/qwen2_5/convert_weight.py", line 94, in convert_pt_to_ms
    ms.save_checkpoint(ckpt_list, output_path)
  File "/home/ma-user/anaconda3/envs/MindSpore/lib/python3.9/site-packages/mindspore/train/serialization.py", line 737, in save_checkpoint
    ckpt_file_name = _check_save_obj_and_ckpt_file_name(save_obj, ckpt_file_name, format)
  File "/home/ma-user/anaconda3/envs/MindSpore/lib/python3.9/site-packages/mindspore/train/serialization.py", line 599, in _check_save_obj_and_ckpt_file_name
    raise IsADirectoryError("For 'save_checkpoint', the parameter `ckpt_file_name`: {} is a directory, "
IsADirectoryError: For 'save_checkpoint', the parameter `ckpt_file_name`: /home/ma-user/work/model_new is a directory, it must be a file name.
(MindSpore) [ma-user mindformers]$
```

MindSpore åœ¨ä¿å­˜è½¬æ¢å¥½çš„æ¨¡å‹æ–‡ä»¶æ—¶ï¼Œæ‚¨é€šè¿‡ `--output_path` å‚æ•°æä¾›äº†ä¸€ä¸ª**æ–‡ä»¶å¤¹è·¯å¾„** (`/home/ma-user/work/model_new`)ï¼Œä½†å®ƒéœ€è¦çš„æ˜¯ä¸€ä¸ª**å®Œæ•´çš„æ–‡ä»¶è·¯å¾„**ï¼ˆåŒ…å«æ–‡ä»¶åï¼‰,å› æ­¤ä¿®æ”¹å‘½ä»¤ä¸ºï¼š

```shell
python convert_weight.py \
--model qwen2_5 \
--input_path /home/ma-user/.cache/modelscope/hub/models/Qwen/Qwen2___5-7B \
--output_path /home/ma-user/work/model_new/qwen2_5_ms.ckpt \
--dtype bf16
```

### 10.1 ç»“æœ

é—®é¢˜å‡ºç°åœ¨äº†

```shell
cd /{path}/mindformers/research/qwen2_5

bash ../../scripts/msrun_launcher.sh "run_qwen2_5.py \
--config /{path}/desired_model_config.yaml \
--run_mode finetune \
--train_data /{path}/alpaca-fastchat4096.mindrecord " 8 PORT output/msrun_log False 2000
```

è¿™ä¸€æ­¥ï¼Œå‘ç°å†





## äºŒã€ mindrlhfè¿è¡Œdeepseekv3æ­¥éª¤









# é™„å½•1

```python
#qwen_preprocess.py

# Copyright 2023 Huawei Technologies Co., Ltd
# Copyright 2024 The Qwen Team, Alibaba Group.
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
# http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.
# ============================================================================
"""
Script to transform wikitext, wikipedia, and QA datasets into MindRecord format for Qwen2.5 models.
"""
import argparse
import json
import os
import numpy as np

from mindspore.mindrecord import FileWriter
from tqdm import tqdm

from mindformers.dataset.dataloader.training_dataloader import TrainingDataset
from mindformers.dataset.dataloader.datareaders import wikitext_clean
# =========================== CHANGE 1: Import Qwen2Tokenizer ===========================
from mindformers.models.qwen2.qwen2_tokenizer import Qwen2Tokenizer
from mindformers.tools import logger

IGNORE_TOKEN_ID = -100

def chunks(lst, n):
    """Yield n sized chunks from list"""
    for i in range(0, len(lst), n):
        yield lst[i:i + n]

def tokenize_wiki(tokenizer, file_path, seq_length, repeat):
    """Tokenize wikitext-2/wikitext-103 dataset"""
    content = []

    total_steps = 5
    with tqdm(total=total_steps, desc="Processing") as pbar:
        pbar.set_description("Reading file")
        with open(file_path, 'r', encoding='utf-8') as f:
            raw_text = f.read()
        pbar.update(1)

        pbar.set_description("Processing text")
        paras = wikitext_clean(raw_text).split("\n\n")
        pbar.update(1)

        pbar.set_description("Tokenizing text")
        # Use a list comprehension for potentially better performance
        all_tokens = [token for para in paras if para and not para.strip().startswith('=')
                      for token in tokenizer(para)['input_ids']]
        content.extend(all_tokens)
        pbar.update(1)

        pbar.set_description("Repeating data")
        if repeat > 1:
            content = content * repeat
        pbar.update(1)

        pbar.set_description("Chunking data")
        for chunk in chunks(content, seq_length):
            if len(chunk) == seq_length:
                yield {'input_ids': np.array(chunk, dtype=np.int32)}
        pbar.update(1)


def tokenize_wikipedia(tokenizer, dataset_dir, seq_length, samples_num):
    """Tokenize wikipedia dataset with parquet format"""
    dataset = TrainingDataset(dataset_dir=dataset_dir,
                              column_names=["input_ids"],
                              max_length=seq_length,
                              file_format="parquet",
                              tokenizer=tokenizer,
                              is_align=True,
                              samples_num=samples_num,
                              shuffle=False)
    for data in dataset:
        input_id_list = data[0]
        if len(input_id_list) == seq_length:
            yield {'input_ids': np.array(input_id_list, dtype=np.int32)}

# =========================== CHANGE 2: Rewrite QA processing for Qwen2 ===========================
def tokenize_qa_for_qwen(tokenizer, file_path, seq_length):
    """Tokenize a QA dataset for Qwen2 using its chat template."""
    try:
        with open(file_path, "r", encoding='utf-8') as f:
            raw_data = json.load(f)
    except Exception as e:
        logger.error("Failed to load or parse JSON file %s: %s", file_path, e)
        return

    # Role mapping for standard conversation formats
    # Qwen's chat template expects roles like 'system', 'user', 'assistant'
    role_map = {"human": "user", "gpt": "assistant"}

    with tqdm(total=len(raw_data), desc="Processing QA data", unit="row") as pbar:
        for example in raw_data:
            conversations = example.get("conversations")
            if not conversations:
                continue

            # Convert to the format expected by apply_chat_template
            messages = [{"role": role_map.get(turn["from"], "user"), "content": turn["value"]}
                        for turn in conversations]

            # Tokenize the entire conversation using the chat template
            # This correctly adds special tokens like <|im_start|> and <|im_end|>
            input_ids = tokenizer.apply_chat_template(
                messages, tokenize=True, add_generation_prompt=False
            )

            # Create labels: we only want to compute loss on the assistant's responses
            labels = list(input_ids)
            
            # Find all assistant messages and mask out non-assistant parts
            assistant_token_id = tokenizer.encode(tokenizer.assistant_token)[-1]
            im_end_token_id = tokenizer.encode(tokenizer.im_end_token)[-1]
            
            is_assistant_turn = False
            for i, token_id in enumerate(labels):
                if token_id == assistant_token_id:
                    is_assistant_turn = True
                
                if not is_assistant_turn:
                    labels[i] = IGNORE_TOKEN_ID
                
                if token_id == im_end_token_id:
                    is_assistant_turn = False
                    labels[i] = IGNORE_TOKEN_ID # Also ignore the <|im_end|> token itself

            # Padding and truncation
            if len(input_ids) > seq_length:
                input_ids = input_ids[:seq_length]
                labels = labels[:seq_length]
            else:
                pad_len = seq_length - len(input_ids)
                input_ids = input_ids + [tokenizer.pad_token_id] * pad_len
                labels = labels + [IGNORE_TOKEN_ID] * pad_len

            yield {
                'input_ids': np.array(input_ids, dtype=np.int32),
                'labels': np.array(labels, dtype=np.int32)
            }
            pbar.update(1)


if __name__ == '__main__':
    parser = argparse.ArgumentParser()
    parser.add_argument('--dataset_type', type=str, required=True, choices=['wiki', 'wikipedia', 'qa'],
                        help="Type of dataset to process.")
    parser.add_argument('--input_glob', type=str, required=True,
                        help="Path to the input data file or directory glob.")
    parser.add_argument('--output_file', type=str, required=True,
                        help="Path for the output MindRecord file (without extension).")
    # =========================== CHANGE 3: Modify argparse for Qwen2 Tokenizer ===========================
    parser.add_argument('--tokenizer_path', type=str, required=True,
                        help="Directory path containing the Qwen2 tokenizer files (e.g., vocab.json, merges.txt).")
    parser.add_argument('--file_partition', type=int, default=1, help="Number of shards for the MindRecord file.")
    parser.add_argument('--repeat', type=int, default=1, help="Number of times to repeat the wiki dataset.")
    parser.add_argument('--samples_num', type=int, default=10000,
                        help="Number of samples to process for 'wikipedia' type.")
    parser.add_argument('--seq_length', type=int, default=4096, help="Sequence length.")
    args = parser.parse_args()

    out_dir, _ = os.path.split(os.path.abspath(args.output_file))
    if not os.path.exists(out_dir):
        os.makedirs(out_dir, exist_ok=True)

    # Define schema based on dataset type
    if args.dataset_type in ['wiki', 'wikipedia']:
        schema = {'input_ids': {"type": "int32", "shape": [-1]}}
    elif args.dataset_type == 'qa':
        schema = {'input_ids': {"type": "int32", "shape": [-1]}, 'labels': {"type": "int32", "shape": [-1]}}
    else:
        raise ValueError(f"Unsupported dataset type: {args.dataset_type}")

    writer = FileWriter(file_name=args.output_file, shard_num=args.file_partition)
    writer.add_schema(schema, f"{args.dataset_type}_dataset")

    # =========================== CHANGE 4: Instantiate Qwen2Tokenizer ===========================
    logger.info("Loading Qwen2 tokenizer from %s...", args.tokenizer_path)
    if not os.path.isdir(args.tokenizer_path):
        raise NotADirectoryError(f"Tokenizer path must be a directory: {args.tokenizer_path}")
    
    # Qwen2Tokenizer is instantiated directly from the directory path
    word_tokenizer = Qwen2Tokenizer.from_pretrained(args.tokenizer_path)
    
    transforms_count = 0
    iterator = None
    if args.dataset_type == 'wiki':
        iterator = tokenize_wiki(word_tokenizer, args.input_glob, args.seq_length, args.repeat)
    elif args.dataset_type == 'wikipedia':
        iterator = tokenize_wikipedia(word_tokenizer, args.input_glob, args.seq_length, samples_num=args.samples_num)
    elif args.dataset_type == 'qa':
        iterator = tokenize_qa_for_qwen(word_tokenizer, args.input_glob, args.seq_length)

    if iterator:
        for x in iterator:
            transforms_count += 1
            writer.write_raw_data([x])
    
    writer.commit()
    print(f"\nTransformation complete. Transformed {transforms_count} records.")
    out_file = args.output_file
    if args.file_partition > 1:
        out_file += '0'
    print(f"Output files can be found at: {out_file}")
```



# é™„å½•2

```yaml
# ======================================================================================
#                            Qwen2.5 7B - 2å¡å¾®è°ƒé…ç½®æ–‡ä»¶
# ======================================================================================

seed: 42
output_dir: './output_qwen2_5_finetune'
# [å¾…å¡«å†™] åŠ è½½é¢„è®­ç»ƒæƒé‡çš„è·¯å¾„ (å®Œæ•´çš„ã€æœªåˆ‡åˆ†çš„ .ckpt æ–‡ä»¶)
load_checkpoint: '[è¯·å¡«å†™æ‚¨çš„å®Œæ•´ckptæ–‡ä»¶è·¯å¾„]/MS_CKPT_NAME.ckpt'
auto_trans_ckpt: True  # è‡ªåŠ¨å°†å®Œæ•´ckptè½¬æ¢ä¸ºåˆ†å¸ƒå¼ckpt

# trainer config
trainer:
  type: CausalLanguageModelingTrainer
  model_name: 'qwen2_5_7b'

# runner config
runner_config:
  epochs: 2
  batch_size: 1
  sink_mode: True
  sink_size: -1 # -1 è¡¨ç¤ºåœ¨ä¸€ä¸ªepochä¸­å…¨é‡ä¸‹æ²‰

# runner wrapper config
runner_wrapper:
  type: MFTrainOneStepCell
  scale_sense:
    type: DynamicLossScaleUpdateCell
    loss_scale_value: 4096
    scale_factor: 2
    scale_window: 1000
  use_clip_grad: True
  max_grad_norm: 1.0

# optimizer
optimizer:
  type: AdamW
  betas: [0.9, 0.95]
  eps: 1.e-8
  learning_rate: 1.e-6
  weight_decay: 0.01

# lr schedule
lr_schedule:
  type: CosineWithWarmUpLR
  learning_rate: 1.e-6
  lr_end: 1.e-7
  warmup_ratio: 0.03
  total_steps: -1 # -1 è¡¨ç¤ºè‡ªåŠ¨è®¡ç®—

# dataset
train_dataset: &train_dataset
  data_loader:
    type: MindDataset
    # [å¾…å¡«å†™] è®­ç»ƒæ•°æ®é›†çš„ .mindrecord æ–‡ä»¶è·¯å¾„
    dataset_dir: "[è¯·å¡«å†™æ‚¨çš„mindrecordæ–‡ä»¶è·¯å¾„]/gsm8k_train.mindrecord"
    shuffle: True
  input_columns: ["input_ids"] # é€šå¸¸å¾®è°ƒåªéœ€è¦ input_ids
  num_parallel_workers: 8
  python_multiprocessing: True
  drop_remainder: True
  batch_size: 1

train_dataset_task:
  type: CausalLanguageModelDataset
  dataset_config: *train_dataset

# parallel config for 2 GPUs
use_parallel: True
parallel:
  parallel_mode: 1 # åŠè‡ªåŠ¨å¹¶è¡Œ
  gradients_mean: False
  enable_alltoall: False
  full_batch: True
  search_mode: "sharding_propagation"
  strategy_ckpt_save_file: "./ckpt_strategy.ckpt"
  enable_parallel_optimizer: True

parallel_config:
  data_parallel: 1
  model_parallel: 2      # <--- 2å¡æ¨¡å‹å¹¶è¡Œ
  pipeline_stage: 1      # <--- æµæ°´çº¿ä¸º1
  use_seq_parallel: False # 2å¡æ—¶é€šå¸¸ä¸å¼€
  micro_batch_num: 16

# recompute config for pipeline_stage = 1
recompute_config:
  recompute: [7] # <--- ä¿®æ­£: åˆ—è¡¨é•¿åº¦å¿…é¡»ä¸º1, å¯¹åº” pipeline_stage=1
  select_recompute:
    'feed_forward\.mul': [14] # <--- ä¿®æ­£: åˆ—è¡¨é•¿åº¦ä»2æ”¹ä¸º1
    'feed_forward\.w1\.activation\.silu': [14] # <--- ä¿®æ­£: åˆ—è¡¨é•¿åº¦ä»2æ”¹ä¸º1
    'feed_forward\.w1\.reshape': [14] # <--- ä¿®æ­£: åˆ—è¡¨é•¿åº¦ä»2æ”¹ä¸º1
    'feed_forward\.w2\.reshape': [14] # <--- ä¿®æ­£: åˆ—è¡¨é•¿åº¦ä»2æ”¹ä¸º1
  mp_comm_recompute: True
  recompute_slice_activation: False

# callbacks
callbacks:
  - type: MFLossMonitor
  - type: CheckpointMonitor
    prefix: "qwen2_5_7b_finetune"
    save_checkpoint_steps: 500
    keep_checkpoint_max: 1
    integrated_save: False

# mindspore context init config
context:
  mode: 0 # Graph Mode
  device_target: "Ascend"
  max_device_memory: "58GB" # è¯·æ ¹æ®æ‚¨çš„NPUæ˜¾å­˜å¤§å°è°ƒæ•´
  save_graphs: False
  device_id: 0
  ascend_config:
    precision_mode: "must_keep_origin_dtype"

# =========================== å…³é”®é”™è¯¯ä¿®æ­£ ===========================
# model config: MUST use Qwen2's config and arch, NOT Llama's
model:
  model_config:
    type: Qwen2Config # <--- ä¿®æ­£: LlamaConfig -> Qwen2Config
    # ä»¥ä¸‹æ˜¯Qwen2-7Bæ¨¡å‹çš„æ ‡å‡†å‚æ•°, æ— éœ€ä¿®æ”¹
    hidden_size: 3584
    num_layers: 28
    num_heads: 28
    num_key_value_heads: 4
    intermediate_size: 18944
    vocab_size: 152064
    max_position_embeddings: 32768
    rope_theta: 1000000.0
    use_flash_attention: True
    compute_dtype: "bfloat16"
    layernorm_compute_type: "float32"
    softmax_compute_type: "float32"
    rotary_dtype: "float32"
    param_init_type: "float32"
    use_past: False
    seq_length: 8192

  arch:
    type: Qwen2ForCausalLM # <--- ä¿®æ­£: LlamaForCausalLM -> Qwen2ForCausalLM
```


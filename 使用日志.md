# 知识储备

### 传统Transformer模型结构

**`Input Embedding` + `Positional Encoding`** -> **`Encoder`** -> **`Decoder`** -> **`线性层/Softmax`**

Encoder和Decoder，又有多个Encoder layer和Decoder layer，里面又有Multi-head-Attention和FNN，带掩码的多头自注意力机制（(回顾自己已经生成的部分)）、交叉注意力机制（ (查询编码器的输出，理解原文)）以及FNN。

**FNN（Feed-Forward Netrwork）**：标准的神经网络，它对自注意力层输出的每个词的表示进行一次独立的、非线性的变换，进一步提取特征。**MoE（混合专家）模型**中，就是将这个巨大的FFN层，替换成了一个路由器和多个小型的“专家”FFN。

#### 现有的主流LLM（Decoder-only）

```
Input (Prompt + 已生成文本)` -> `Input Embedding + Positional Encoding` -> `Decoder` -> `线性层/Softmax
```

**`Decoder`**，由多个 `Decoder layer` 堆叠而成，里面主要有：**`带掩码的多头自注意力机制`**（统一理解Prompt和已生成内容的全局上下文，是模型中**唯一的**注意力机制）和 **`FNN`**（对每个词的表示进行独立的非线性变换，深入提取特征）。

​	**编码器 (Encoder) 没了**：整个负责“深度理解原文”的独立部门被裁撤了。

​	**交叉注意力机制 (Cross-Attention) 没了**：因为编码器这个“信息源”不存在了，所以解码器内部那个负责“查询原文理解”的交叉注意力机制自然也就没有存在的必要了。

​	**带掩码的自注意力机制 (Masked Self-Attention) 承担了全部职责**：这个模块现在是模型的核心。它既要负责**“向后看”**，深度理解用户输入的Prompt以及自己已经生成的所有内容，又要负责**“向前看”**（预测下一个词），统一了理解和生成的双重任务。

### **什么是MoE？**

传统的LLM是将所有的知识都塞到整个参数里面，这就导致每次回答问题都要调动全部的参数。

MoE采用分而治之的思想，在处理任何一个token时，实际被激活参与计算的**“有效参数量”**非常小（只有被选中的K个专家的参数）

分为 Experts 专家网络和 Gating Router 门控网络，

**工作流程**

一个输入token进入MoE层。

**路由器（Gating Network）**对这个token进行分析，并输出一个权重列表，决定将这个任务分配给哪些专家，以及每个专家的权重是多少。

通常系统会选择**Top-K**（K通常很小，比如1或2）个得分最高的专家。

这个token的数据**只被发送给这K个被选中的专家**进行计算。

**所有其他的专家网络在这次计算中保持“沉默”，完全不消耗计算资源。**

K个专家的输出结果，根据路由器给出的权重进行加权求和，形成最终的输出。

### 并行策略

##### **DP – Data Parallelism 数据并行**

- 每个设备（GPU/NPU）都放一份完整模型，切分 **batch 数据**。
- 每个 rank 只算自己这部分样本的 loss 和梯度。
- 通过 **all-reduce** 把梯度同步，然后更新模型参数。
   👉 优点：实现简单，扩展性好。缺点：显存需求随模型大小增长。

------

##### **TP – Tensor Parallelism 张量并行**

- 把 **单层里的矩阵乘法（比如 WQ/WK/WV, FFN）** 按列/行切分到不同设备计算。
- 每个设备持有一部分权重，前向/反向过程中需要 all-reduce 聚合。
   👉 优点：可以训练超大模型（参数切开了）。缺点：通信频繁，带宽压力大。

------

##### **PP – Pipeline Parallelism 流水线并行**

- 把模型的 **层（layer）** 切分成几个 stage，每个 stage 放在不同设备。
- 前向时像工厂流水线：样本在 stage1 → stage2 → stage3 传递。
- 为避免空转，常用 **micro-batch** 切分，保证流水线“满负荷”。
   👉 优点：突破显存瓶颈，减少单卡模型大小。缺点：调度复杂，有 pipeline bubble。

------

##### **CP – Context Parallelism 上下文并行**

- 针对 **自注意力 (Self-Attention)** 的 KV 矩阵，把序列长度维度切分。
- 每个设备只存部分 token 的 KV，计算时需要跨设备通信。
   👉 优点：缓解长序列注意力显存瓶颈。

------

##### **EP – Expert Parallelism 专家并行（MoE 专用）**

- 在 Mixture-of-Experts (MoE) 模型里，不同专家（FFN 模块）放在不同设备。
- 路由器 (router) 把 token 分配到对应专家上计算。
   👉 优点：参数量巨大但计算量可控。缺点：负载均衡和通信复杂。

------

##### 2. ZeRO (Zero Redundancy Optimizer)

这是 DeepSpeed 提出的显存优化方法，把 **优化器状态 / 梯度 / 模型参数** 在设备间切分，而不是每个 rank 存一份：

- **Stage 1**：优化器状态分片（optimizer states sharding）
- **Stage 2**：再加上梯度分片（gradient sharding）
- **Stage 3**：再加上参数分片（parameter sharding）
- **Stage 3+**：加上 offload（显存不足时把部分内容放 CPU/NVMe）

👉 本质：用 **通信换显存**，每张卡只存自己负责的部分，训练时通过聚合/广播来完成计算。

------

##### 3. Activation Recomputation (激活重计算)

- 正常训练时需要保存 **每一层的中间激活值**（forward 结果），反向时才能用。
- 但激活占显存非常大（尤其长序列）。
- **激活重计算**：只保存一部分关键节点，其余激活在反向传播时 **重新计算一次 forward** 得到。
   👉 牺牲 **算力（多一次 forward 计算）**，换取 **显存降低**。

------

##### 4. Offloading

- 当显存（GPU/NPU memory）不够时，把一些数据转移到 **CPU 内存** 或 **NVMe SSD**。
- 常见 offload 对象：
  - **优化器状态**（Adam 的 m/v 向量很大）
  - **参数**
  - **激活值**
- 框架如 DeepSpeed ZeRO-Offload 就支持这种方式。
   👉 用 **带宽换显存**，训练速度下降，但能跑更大模型。

### 什么是DeepSeppd？

是微软开源的一个大规模深度学习训练优化框架。

核心功能：

1. ZeRO优化器(Zero Redundancy Optimizer)，用分片的方式吧优化器的状态/梯度/参数拆分到不同的GPU上，避免重复存储。

2. 并行训练支持。支持DP TP PP，并且可以跟ZeRO结合。还支持MoEt的EP。还可以跟Megatron-LM集成。

3. 显存优化。

   ​	**Activation Checkpointing (激活重计算)**：减少保存的中间结果，用算力换显存。

   ​	**Offloading**：把优化器/参数搬到 CPU 或 NVMe。

   ​	**量化训练 (FP16, BF16, 8bit optimizer)**。

## AReal

rollout和train完全解耦，rollout worker一直生成样本，traine worker只要收集到足够的样本就进行训练。

## Parital Rollout

启动更多的rollout请求，当完成部分请求之后就进行训练，剩余的rollout请求异步完成或者缓存。



## Megetron

出自 **NVIDIA**，是一个 **大规模 Transformer 训练框架**，关键技术有TP PP SP/CP 融合算子。

常常和DeepSpeed结合使用

## DeepSpeed

出自 **微软 (Microsoft)**，是一个 **大规模分布式训练和推理优化库**。关键技术有ZeRO（把优化器状态 / 梯度 / 参数分片存放到不同 GPU，极大节省显存）

常常和Megetron结合使用

## SGLang

用来运行LLM并控制其输出（输出格式等）的编程语言和引擎



**LLaMA** 系列使用 **SentencePiece** Tokenizer，其文件通常是 `tokenizer.model`。

**Qwen2.5** 系列使用基于 **BPE** 的 Tokenizer，其文件通常是 `vocab.json`, `merges.txt`, 和 `tokenizer.json`



## 现有框架总结

AReal

VeRL

slime

### ROLL

![image-20250914151806223](进度安排.assets/image-20250914151806223.png)

一个中央控制器来统一协调管理。将底层的并行计算任务抽象成统一的并行工作单元（parallel worker）。

ROLL接收用户定义的RL**数据流图（dataflow graph）**及其相关配置作为输入。基于此输入，**分布式执行器与调度器（distributed executor and scheduler）**负责协调各个工作单元和调度器。**自动设备映射（AutoDeviceMapping）**模块管理已配置的资源池**（resource Pool）**中的资源，并高效地将工作单元和调度器绑定到其分配的资源上。

**并行策略 (Parallel Strategy)**。ROLL中的RL训练包含训练、推理和生成三个阶段。我们集成了**MegatronCore**和**DeepSpeed**来加速LLM训练，支持包括DP、PP、TP、CP和EP在内的先进5D并行策略。得益于DeepSpeed，ROLL还支持ZeRO2、ZeRO3和ZeRO-offload。此外，我们提供梯度检查点和卸载策略以显著减少GPU内存消耗，从而能够在资源受限的设备上高效执行。对于推理和生成阶段，我们集成了**vLLM**和**SGLang**，为ROLL配备了TP、EP、PP来加速推理和生成阶段。



**Rollout Scheduler：**The Rollout Scheduler allows users to schedule the lifecycle of each request at the granularity of **individual samples, rather than batches**, during the generation stage. Particularly, the Rollout Scheduler can **dynamically add and abort requests based on current resource availability and the progress of response generation.**

AutoDeviceMapping and Resource Pool：自动设备映射模块协调资源池中的一组CPU和GPU资源，并将它们绑定到工作单元和调度器上。

![image-20250914151819119](进度安排.assets/image-20250914151819119.png)





### Rlinf

代码开源但论文未发布。多了M2Flow（(Macro-to-Micro Flow）

把宏观逻辑和微观执行解耦。

宏观逻辑：逻辑工作流的构建，例如

```python
# 1. 生成阶段
responses = actor_model.generate(prompts)

# 2. 推理/打分阶段
rewards = reward_model.score(responses)

# 3. 训练阶段
actor_model.update(responses, rewards)
```

微观逻辑：系统是如何高效完成具体的执行计划的。物理通信调度；数据如何切分，如何打包，通过哪个网络路径发送等。

# 整个框架

RL训练主要包括generation (inference)和training两个阶段，generation需要搞笑的KV cache管理和解码内核；train需要DP TP PP等等。他们需要不同的最优并行化策略。





# 日志

## 一、mindrlhf运行Qwen2.5步骤（搁置-10.1）

## 0. 整体思路

**（1）准备资源**

下载`mindRLHF`和`mindFormers`两个核心代码库；

获取预训练模型；

获取数据集；

配置环境。

**（2）处理模型**

运行`convert_weight.py`脚本，将pytorch的权重文件转换成mindspore支持的、单文件的`.ckpt`格式；

生成分布式策略文件，

- 目的是生成一个指导文件，告诉程序如何将一个LLM分到2张或多张卡上；
- 运行一个短时间的、伪造的微调任务（使用alpaca数据集），让mindspore吧计算图构建出来，并根据自己设置的2卡或者多卡并行配置，自动保存这份“图纸”（策略文件）；
- 执行权重切分，运行`transform_checkpoint.py`脚本，依据上一步生成的“图纸”，将第一步得到的那个完整的`.ckpt`文件，切分成2份或者多份分布式权重文件

**（3）处理数据**

目的：原始的`.jsonl`格式文件读取效率低，需要转换为mindspore支持的`.mindrecord`格式文件；

运行`rlhf_data.py`脚本，将`GSM8K`的`train.jsonl`文件，连同Qwen2.5的tokenizer一起处理，生成最终用于训练的`gsm8k_train.mindrecord`文件。

**（4）编写与核对配置文件**

模型并行配置文件

- 定义模型架构

- 定义2卡的并行策略

- 定义相关的重计算等优化策略

GRPO算法配置文件

- 定义GRPO算法的超参数等等

**（5）启动训练**

设置 `PYTHONPATH`：运行 `export` 命令，告诉 Python 去哪里找 `mindformers` 和 `mindrlhf` 的代码。

执行 `msrun` 命令：

- 这是最终的训练启动命令。
- 它会使用 **2 卡** (`--worker_num=2`) 来运行。
- 它会加载**第三步**处理好的 `gsm8k` 数据集。
- 它会加载**第二步**切分好的 `2卡` 分布式权重。
- 它会遵循**第四步**配置好的所有训练参数。
- 最终开始真正的 GRPO 算法微调，并将训练好的模型保存下来。







### 1. 模型以及数据集获取与预处理

下面是正式的步骤

####  1.1 模型权文件和tokenizer获取

```shell
mkdir models
cd models
mkdir Qwen2.5
mkdir Qwen2.5_new1 
pip install modelscope
modelscope download --model Qwen/Qwen2.5-7B  --local_dir   /home/ma-user/work/models/Qwen2.5


git clone https://gitee.com/mindspore/mindrlhf.git

git clone https://gitee.com/mindspore/mindformers.git
cd mindformers
bash build.sh

cd mindformers/research/qwen2_5

python convert_weight.py \
--torch_ckpt_dir /home/ma-user/work/models/Qwen2.5 \
--mindspore_ckpt_path /home/ma-user/work/models/qwen2_5_ms.ckpt \
--dtype bf16 \
--config_path /home/ma-user/work/mindformers/research/qwen2_5/finetune_qwen2_5_7b_8k.yaml
```

####  1.2 模型权重离线切分

使用多卡分布式训练时，手动进行分布式权重切分，先生成切分方案，在执行切分动作。

先获取数据文件/{path}/alpaca-fastchat4096.mindrecord，步骤如下：

**v1（废弃）**

```shell
建议新开个terminal
pip install fastchat>=0.2.13     运行不了就把>=0.2.13删了

git clone https://github.com/tatsu-lab/stanford_alpaca.git
cd /home/ma-user/work/mindformers/mindformers/tools/dataset_preprocess/llama

cd work
mkdir data
cd mindformers/tools/dataset_preprocess/llama
python alpaca_converter.py \
--data_path /home/ma-user/work/stanford_alpaca/alpaca_data.json \
--output_path /home/ma-user/work/data/alpaca-data-conversation.json


cd mindformers/tools/dataset_preprocess/llama

此处需要在mindformers/tools/dataset_preprocess/llama下新建一个qwen_preprocess.py   见最上面


python qwen_preprocess.py \
--dataset_type qa \
--input_glob /home/ma-user/work/data/alpaca-data-conversation.json \
--model_file /home/ma-user/work/models/Qwen2.5/tokenizer.json \
--seq_length 4096 \
--output_file /home/ma-user/work/data/alpaca-fastchat4096.mindrecord

```

**v2**

```shell
pip install fastchat
git clone https://github.com/tatsu-lab/stanford_alpaca.git   获取alpaca 数据集

cd mindformers/research/qwen2/


# 执行research/qwen2/alpaca_converter.py，将原始数据集转换为指定格式。
python alpaca_converter.py \
--data_path /home/ma-user/work/stanford_alpaca/alpaca_data.json \
--output_path /home/ma-user/work/data/alpaca-data-conversation.json 
 
# 执行research/qwen2/qwen2_preprocess.py文件，进行数据预处理和Mindrecord数据生成。 
# 注意 需要修改一下qwen2_preprocess的代码
# 打开 qwen2_preprocess.py
# /home/ma-user/work/mindformers/research/qwen2/qwen2_preprocess.py
# 修改导入行
# 将第 27 行的绝对导入修改为相对导入。
#
# 原来的代码:
# from research.qwen2.qwen2_tokenizer import Qwen2Tokenizer
# 修改为:
# from qwen2_tokenizer import Qwen2Tokenizer

python qwen2_preprocess.py \
 --dataset_type 'qa' \
 --input_glob /home/ma-user/work/data/alpaca-data-conversation.json \
 --vocab_file /home/ma-user/work/models/Qwen2.5/vocab.json \
 --merges_file /home/ma-user/work/models/Qwen2.5/merges.txt \
 --seq_length 4096 \
 --output_file /home/ma-user/work/data/alpaca-fastchat4096.mindrecord
```



接着进行生成切分方案，在执行切分动作。

```shell
先生成切分方案，在执行切分动作
生成切分方案时，msrun_launcher.sh默认是8卡，需要手动修改


cd /research/qwen2_5

# 由于生成切分方案时，msrun_launcher.sh默认是8卡，需要手动修改  下面是手动修改的步骤
cd /home/ma-user/work/mindformers/research/qwen2_5/
cp finetune_qwen2_5_7b_8k.yaml finetune_qwen2_5_7b_2cards.yaml
# 修改后的详细代码见附录2


bash ../../scripts/msrun_launcher.sh "run_qwen2_5.py \
--config /home/ma-user/work/mindformers/research/qwen2_5/finetune_qwen2_5_7b_2cards.yaml \
--run_mode finetune \
--train_data /home/ma-user/work/data/alpaca-fastchat4096.mindrecord " 2 PORT output/msrun_log False 2000


注意：PORT 是一个占位符 (Placeholder)，是教程里提醒您需要手动替换成一个具体数字的地方   需要选择一个当前未被占用的端口号（通常选择 1024 到 65535 之间的任意数字，比如教程里常用的 9887），然后用这个数字替换掉命令中的 PORT。

# 这个命令会在/research/qwen2_5/strategy路径下生成并行策略文件，在下一步切分ckpt时作为dst_strategy的值传入。
# 查看报错日志tail -f /home/ma-user/work/mindformers/research/qwen2_5/output/msrun_log/worker_0.log
```

【这里少一步，少了切片的代码，因为上面这一步一直在报错】







#### 1.3 数据集文件获取与预处理

```shell
建议新开个terminal
cd work
# 下载gsm8k数据集
git clone https://github.com/openai/grade-school-math.git

# 下载完成后，需要转为MindSpore使用的.mindrecord文件   首先进入MindRLHF路径 并执行以下脚本：
cd /mindrlhf

pip install jsonlines

# 注意，这里需要添加环境变量，见3. 启动GRPO训练脚本


python examples/grpo/qwen_grpo_tutorial/rlhf_data.py \
--vocab_path /home/ma-user/work/models/Qwen2.5/vocab.json \
--merges_file_path /home/ma-user/work/models/Qwen2.5/merges.txt \
--file_path /home/ma-user/work/grade-school-math/grade_school_math/data/train.jsonl \
--output_path /home/ma-user/work/data/gsm8k_train.mindrecord \
--dataset_type gsm8k



```

## 2. GRPO算法及模型配置

#### 2.1  训练/推理模型配置

训练模型的配置文件默认为`model_configs/qwen_grpo/qwen2_5_7b/finetune_qwen2_5_7b.yaml`，其中用户可以手动配置训练模型的并行策略：

```yaml
parallel_config:
    data_parallel: 1
    model_parallel: 4
    pipeline_stage: 2
    use_seq_parallel: True
    micro_batch_num: 2
    micro_batch_interleave_num: 2
# 参数说明
data_parallel:                数据并行切分组数
model_parallel:               模型并行(tensor parallel)切分组数
pipeline_stage:               流水线并行切分组数
use_seq_parallel:             是否使用sequence parallel
micro_batch_num:              流水线并行中的micro batch number
micro_batch_interleave_num:   当model_parallel>1时,可以设置为2以加速训练
```

推理模型的配置文件默认为`model_configs/qwen_grpo/qwen2_5_7b/predict_qwen2_5_7b_instruct.yaml`，其中用户可以手动配置推理模型的并行策略：

```yaml
parallel_config:
    data_parallel: 2
    model_parallel: 4
    pipeline_stage: 1
# 参数说明
data_parallel:                数据并行切分组数
model_parallel:               模型并行(tensor parallel)切分组数
pipeline_stage:               流水线并行切分组数, 必须为1. 当前推理模型不支持流水线并行
```

#### 2.2  GRPO训练算法配置

GRPO训练算法相关配置可以在`examples/grpo/qwen_grpo_tutorial/grpo_config.yaml`内进行修改，包括以下参数：

```yaml
beta: float = 0.01
num_generations: int = 8
num_rollouts: int = 4
epochs: int = 2
start_lr: float = 5e-7
end_lr: float = 1e-10
chunk_size: int = 2
batch_size: int = 2
sync_ref_model: bool = True
ref_model_sync_steps: int = 50

# 参数说明
beta:                   反向训练GRPO loss中KL散度的权重
num_generations:        推理模型在每一步中为每个问题生成多少个回答
num_rollouts:           推理模型在训练之前会反复进行多少轮
epochs:                 在数据集上总共训练的epochs轮数
start_lr:               初始时反向训练的learning rate步长
end_lr:                 结束时反向训练的learning rate步长, 必须严格小于于start_lr
chunk_size:             推理模型在每一步中为多少个问题生成回答
batch_size:             反向训练的batch size
sync_ref_model:         是否每隔若干步将ref model的权重更新为最新的训练模型权重
ref_model_sync_steps:   若sync_ref_model=True, ref model权重更新的间隔步数
```

#### 2.3 GRPO性能统计配置

GRPO性能统计相关配置可以在`examples/grpo/qwen_grpo_tutorial/grpo_config.yaml`内进行修改，包括以下参数：

```
performance_stats: bool = False

# 参数说明
performance_stats:                   是否在日志中打印各流程的执行时间
```

### 3. 启动GRPO训练脚本

首先进入MindRLHF路径

```shell
# 单开个terminal
cd mindrlhf
```

使用以下命令将本地mindrlhf和mindformers代码库均加入PYTHONPATH，MINDFORMERS_PAT路径中

```shell
MINDRLHF_FILE=/{path}/mindrlhf/
MINDFORMERS_FILE=/{path}/mindformers/

export PYTHONPATH="$MINDRLHF_FILE:$MINDFORMERS_FILE:$PYTHONPATH"
export MINDFORMERS_PATH="$MINDFORMERS_FILE $MINDFORMERS_PATH"
```

### 单机8卡拉起Qwen2.5-7b

随后使用以下命令拉起单机8卡GRPO训练任务，可以参考run_grpo.sh

```yaml
msrun --worker_num=8 --local_worker_num=8 \
--master_addr=127.0.0.1 --master_port=9887 \
--join=True --log_dir=./prof_vllm_log \
examples/grpo/qwen_grpo_tutorial/main.py \
--config examples/grpo/qwen_grpo_tutorial/grpo_config.yaml \
--dataset_file /path/to/datasets/gsm8k_train.mindrecord \
--tokenizer_dir /path/to/configs/ \
--actor_checkpoint_path /path/to/weights/qwen2_5_7b/ \
--ref_checkpoint_path /path/to/weights/qwen2_5_7b/ \
--generate_checkpoint_path /path/to/weights/qwen2_5_7b/ \
--verifier_function "qwen_accuracy_reward,format_reward" \
--verifier_weight "1.0,1.0" \
--save_checkpoint_dir "/path/to/ckpt/"


# 在运行此命令前，请确保您已经设置好了 PYTHONPATH 环境变量
# export PYTHONPATH="/home/ma-user/work/mindrlhf:/home/ma-user/work/mindformers:$PYTHONPATH"

msrun --worker_num=8 --local_worker_num=8 \
--master_addr=127.0.0.1 --master_port=9887 \
--join=True --log_dir=./prof_vllm_log \
examples/grpo/qwen_grpo_tutorial/main.py \
--config examples/grpo/qwen_grpo_tutorial/grpo_config.yaml \
--dataset_file /home/ma-user/work/data/gsm8k_train.mindrecord \
--tokenizer_dir /home/ma-user/work/models/Qwen2.5/ \
--actor_checkpoint_path /home/ma-user/work/weights/qwen2_5_7b_distributed/ \
--ref_checkpoint_path /home/ma-user/work/weights/qwen2_5_7b_distributed/ \
--generate_checkpoint_path /home/ma-user/work/weights/qwen2_5_7b_distributed/ \
--verifier_function "qwen_accuracy_reward,format_reward" \
--verifier_weight "1.0,1.0" \
--save_checkpoint_dir "/home/ma-user/work/output/grpo_qwen_7b_ckpt/"


# 参数说明
# msrun 参数
worker_num:                   总卡数
local_worker_num:             单机的卡数
master_addr:                  主节点地址
master_port:                  主节点端口
join:                         是否等待所有worker退出
log_dir:                      日志路径
# main.py 参数
config:                       grpo的配置文件
tokenizer_dir:                模型对应的tokenizer文件vocab.json和merges.txt所在的目录
dataset_file:                 训练数据集mindrecord文件的路径
save_checkpoint_dir:          训练ckpt的保存路径
generate_checkpoint_path:        推理模型(分布式)ckpt文件路径
actor_checkpoint_path:       训练模型(分布式)ckpt文件路径
ref_checkpoint_path:          参考模型(分布式)ckpt文件路径
verifier_function:                 reward函数
verifier_weight:               reward的权重系数
tensorboard_dir:              tensorboard落盘路径，仅在需要使用tensorboard记录时开启
tensorboard_queue_size:       tensorboard缓存队列大小
```









### 9.30结果

**问题1**

mindspore2.2.0版本在运行convert_weight.py报错，原因是因为mindspore版本太低，因此

```shell
pip install --upgrate minspore
```

更新到了2.7.0之后可以正常运行进行权重转换了（转换成MindSpore使用的.ckpt文件）

**问题2**

```
Traceback (most recent call last):
  File "/home/ma-user/work/mindformers/convert_weight.py", line 101, in <module>
    convert_func(merged_args)
  File "/home/ma-user/work/mindformers/research/qwen2_5/convert_weight.py", line 316, in convert_weight
    convert_pt_to_ms(input_path=para.torch_ckpt_dir, output_path=para.mindspore_ckpt_path, dtype=dtype)
  File "/home/ma-user/work/mindformers/research/qwen2_5/convert_weight.py", line 94, in convert_pt_to_ms
    ms.save_checkpoint(ckpt_list, output_path)
  File "/home/ma-user/anaconda3/envs/MindSpore/lib/python3.9/site-packages/mindspore/train/serialization.py", line 737, in save_checkpoint
    ckpt_file_name = _check_save_obj_and_ckpt_file_name(save_obj, ckpt_file_name, format)
  File "/home/ma-user/anaconda3/envs/MindSpore/lib/python3.9/site-packages/mindspore/train/serialization.py", line 599, in _check_save_obj_and_ckpt_file_name
    raise IsADirectoryError("For 'save_checkpoint', the parameter `ckpt_file_name`: {} is a directory, "
IsADirectoryError: For 'save_checkpoint', the parameter `ckpt_file_name`: /home/ma-user/work/model_new is a directory, it must be a file name.
(MindSpore) [ma-user mindformers]$
```

MindSpore 在保存转换好的模型文件时，您通过 `--output_path` 参数提供了一个**文件夹路径** (`/home/ma-user/work/model_new`)，但它需要的是一个**完整的文件路径**（包含文件名）,因此修改命令为：

```shell
python convert_weight.py \
--model qwen2_5 \
--input_path /home/ma-user/.cache/modelscope/hub/models/Qwen/Qwen2___5-7B \
--output_path /home/ma-user/work/model_new/qwen2_5_ms.ckpt \
--dtype bf16
```

### 10.1 结果

问题出现在了

```shell
cd /{path}/mindformers/research/qwen2_5

bash ../../scripts/msrun_launcher.sh "run_qwen2_5.py \
--config /{path}/desired_model_config.yaml \
--run_mode finetune \
--train_data /{path}/alpaca-fastchat4096.mindrecord " 8 PORT output/msrun_log False 2000
```

这一步，发现再





## 二、 mindrlhf运行deepseekv3步骤









# 附录1

```python
#qwen_preprocess.py

# Copyright 2023 Huawei Technologies Co., Ltd
# Copyright 2024 The Qwen Team, Alibaba Group.
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
# http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.
# ============================================================================
"""
Script to transform wikitext, wikipedia, and QA datasets into MindRecord format for Qwen2.5 models.
"""
import argparse
import json
import os
import numpy as np

from mindspore.mindrecord import FileWriter
from tqdm import tqdm

from mindformers.dataset.dataloader.training_dataloader import TrainingDataset
from mindformers.dataset.dataloader.datareaders import wikitext_clean
# =========================== CHANGE 1: Import Qwen2Tokenizer ===========================
from mindformers.models.qwen2.qwen2_tokenizer import Qwen2Tokenizer
from mindformers.tools import logger

IGNORE_TOKEN_ID = -100

def chunks(lst, n):
    """Yield n sized chunks from list"""
    for i in range(0, len(lst), n):
        yield lst[i:i + n]

def tokenize_wiki(tokenizer, file_path, seq_length, repeat):
    """Tokenize wikitext-2/wikitext-103 dataset"""
    content = []

    total_steps = 5
    with tqdm(total=total_steps, desc="Processing") as pbar:
        pbar.set_description("Reading file")
        with open(file_path, 'r', encoding='utf-8') as f:
            raw_text = f.read()
        pbar.update(1)

        pbar.set_description("Processing text")
        paras = wikitext_clean(raw_text).split("\n\n")
        pbar.update(1)

        pbar.set_description("Tokenizing text")
        # Use a list comprehension for potentially better performance
        all_tokens = [token for para in paras if para and not para.strip().startswith('=')
                      for token in tokenizer(para)['input_ids']]
        content.extend(all_tokens)
        pbar.update(1)

        pbar.set_description("Repeating data")
        if repeat > 1:
            content = content * repeat
        pbar.update(1)

        pbar.set_description("Chunking data")
        for chunk in chunks(content, seq_length):
            if len(chunk) == seq_length:
                yield {'input_ids': np.array(chunk, dtype=np.int32)}
        pbar.update(1)


def tokenize_wikipedia(tokenizer, dataset_dir, seq_length, samples_num):
    """Tokenize wikipedia dataset with parquet format"""
    dataset = TrainingDataset(dataset_dir=dataset_dir,
                              column_names=["input_ids"],
                              max_length=seq_length,
                              file_format="parquet",
                              tokenizer=tokenizer,
                              is_align=True,
                              samples_num=samples_num,
                              shuffle=False)
    for data in dataset:
        input_id_list = data[0]
        if len(input_id_list) == seq_length:
            yield {'input_ids': np.array(input_id_list, dtype=np.int32)}

# =========================== CHANGE 2: Rewrite QA processing for Qwen2 ===========================
def tokenize_qa_for_qwen(tokenizer, file_path, seq_length):
    """Tokenize a QA dataset for Qwen2 using its chat template."""
    try:
        with open(file_path, "r", encoding='utf-8') as f:
            raw_data = json.load(f)
    except Exception as e:
        logger.error("Failed to load or parse JSON file %s: %s", file_path, e)
        return

    # Role mapping for standard conversation formats
    # Qwen's chat template expects roles like 'system', 'user', 'assistant'
    role_map = {"human": "user", "gpt": "assistant"}

    with tqdm(total=len(raw_data), desc="Processing QA data", unit="row") as pbar:
        for example in raw_data:
            conversations = example.get("conversations")
            if not conversations:
                continue

            # Convert to the format expected by apply_chat_template
            messages = [{"role": role_map.get(turn["from"], "user"), "content": turn["value"]}
                        for turn in conversations]

            # Tokenize the entire conversation using the chat template
            # This correctly adds special tokens like <|im_start|> and <|im_end|>
            input_ids = tokenizer.apply_chat_template(
                messages, tokenize=True, add_generation_prompt=False
            )

            # Create labels: we only want to compute loss on the assistant's responses
            labels = list(input_ids)
            
            # Find all assistant messages and mask out non-assistant parts
            assistant_token_id = tokenizer.encode(tokenizer.assistant_token)[-1]
            im_end_token_id = tokenizer.encode(tokenizer.im_end_token)[-1]
            
            is_assistant_turn = False
            for i, token_id in enumerate(labels):
                if token_id == assistant_token_id:
                    is_assistant_turn = True
                
                if not is_assistant_turn:
                    labels[i] = IGNORE_TOKEN_ID
                
                if token_id == im_end_token_id:
                    is_assistant_turn = False
                    labels[i] = IGNORE_TOKEN_ID # Also ignore the <|im_end|> token itself

            # Padding and truncation
            if len(input_ids) > seq_length:
                input_ids = input_ids[:seq_length]
                labels = labels[:seq_length]
            else:
                pad_len = seq_length - len(input_ids)
                input_ids = input_ids + [tokenizer.pad_token_id] * pad_len
                labels = labels + [IGNORE_TOKEN_ID] * pad_len

            yield {
                'input_ids': np.array(input_ids, dtype=np.int32),
                'labels': np.array(labels, dtype=np.int32)
            }
            pbar.update(1)


if __name__ == '__main__':
    parser = argparse.ArgumentParser()
    parser.add_argument('--dataset_type', type=str, required=True, choices=['wiki', 'wikipedia', 'qa'],
                        help="Type of dataset to process.")
    parser.add_argument('--input_glob', type=str, required=True,
                        help="Path to the input data file or directory glob.")
    parser.add_argument('--output_file', type=str, required=True,
                        help="Path for the output MindRecord file (without extension).")
    # =========================== CHANGE 3: Modify argparse for Qwen2 Tokenizer ===========================
    parser.add_argument('--tokenizer_path', type=str, required=True,
                        help="Directory path containing the Qwen2 tokenizer files (e.g., vocab.json, merges.txt).")
    parser.add_argument('--file_partition', type=int, default=1, help="Number of shards for the MindRecord file.")
    parser.add_argument('--repeat', type=int, default=1, help="Number of times to repeat the wiki dataset.")
    parser.add_argument('--samples_num', type=int, default=10000,
                        help="Number of samples to process for 'wikipedia' type.")
    parser.add_argument('--seq_length', type=int, default=4096, help="Sequence length.")
    args = parser.parse_args()

    out_dir, _ = os.path.split(os.path.abspath(args.output_file))
    if not os.path.exists(out_dir):
        os.makedirs(out_dir, exist_ok=True)

    # Define schema based on dataset type
    if args.dataset_type in ['wiki', 'wikipedia']:
        schema = {'input_ids': {"type": "int32", "shape": [-1]}}
    elif args.dataset_type == 'qa':
        schema = {'input_ids': {"type": "int32", "shape": [-1]}, 'labels': {"type": "int32", "shape": [-1]}}
    else:
        raise ValueError(f"Unsupported dataset type: {args.dataset_type}")

    writer = FileWriter(file_name=args.output_file, shard_num=args.file_partition)
    writer.add_schema(schema, f"{args.dataset_type}_dataset")

    # =========================== CHANGE 4: Instantiate Qwen2Tokenizer ===========================
    logger.info("Loading Qwen2 tokenizer from %s...", args.tokenizer_path)
    if not os.path.isdir(args.tokenizer_path):
        raise NotADirectoryError(f"Tokenizer path must be a directory: {args.tokenizer_path}")
    
    # Qwen2Tokenizer is instantiated directly from the directory path
    word_tokenizer = Qwen2Tokenizer.from_pretrained(args.tokenizer_path)
    
    transforms_count = 0
    iterator = None
    if args.dataset_type == 'wiki':
        iterator = tokenize_wiki(word_tokenizer, args.input_glob, args.seq_length, args.repeat)
    elif args.dataset_type == 'wikipedia':
        iterator = tokenize_wikipedia(word_tokenizer, args.input_glob, args.seq_length, samples_num=args.samples_num)
    elif args.dataset_type == 'qa':
        iterator = tokenize_qa_for_qwen(word_tokenizer, args.input_glob, args.seq_length)

    if iterator:
        for x in iterator:
            transforms_count += 1
            writer.write_raw_data([x])
    
    writer.commit()
    print(f"\nTransformation complete. Transformed {transforms_count} records.")
    out_file = args.output_file
    if args.file_partition > 1:
        out_file += '0'
    print(f"Output files can be found at: {out_file}")
```



# 附录2

```yaml
# ======================================================================================
#                            Qwen2.5 7B - 2卡微调配置文件
# ======================================================================================

seed: 42
output_dir: './output_qwen2_5_finetune'
# [待填写] 加载预训练权重的路径 (完整的、未切分的 .ckpt 文件)
load_checkpoint: '[请填写您的完整ckpt文件路径]/MS_CKPT_NAME.ckpt'
auto_trans_ckpt: True  # 自动将完整ckpt转换为分布式ckpt

# trainer config
trainer:
  type: CausalLanguageModelingTrainer
  model_name: 'qwen2_5_7b'

# runner config
runner_config:
  epochs: 2
  batch_size: 1
  sink_mode: True
  sink_size: -1 # -1 表示在一个epoch中全量下沉

# runner wrapper config
runner_wrapper:
  type: MFTrainOneStepCell
  scale_sense:
    type: DynamicLossScaleUpdateCell
    loss_scale_value: 4096
    scale_factor: 2
    scale_window: 1000
  use_clip_grad: True
  max_grad_norm: 1.0

# optimizer
optimizer:
  type: AdamW
  betas: [0.9, 0.95]
  eps: 1.e-8
  learning_rate: 1.e-6
  weight_decay: 0.01

# lr schedule
lr_schedule:
  type: CosineWithWarmUpLR
  learning_rate: 1.e-6
  lr_end: 1.e-7
  warmup_ratio: 0.03
  total_steps: -1 # -1 表示自动计算

# dataset
train_dataset: &train_dataset
  data_loader:
    type: MindDataset
    # [待填写] 训练数据集的 .mindrecord 文件路径
    dataset_dir: "[请填写您的mindrecord文件路径]/gsm8k_train.mindrecord"
    shuffle: True
  input_columns: ["input_ids"] # 通常微调只需要 input_ids
  num_parallel_workers: 8
  python_multiprocessing: True
  drop_remainder: True
  batch_size: 1

train_dataset_task:
  type: CausalLanguageModelDataset
  dataset_config: *train_dataset

# parallel config for 2 GPUs
use_parallel: True
parallel:
  parallel_mode: 1 # 半自动并行
  gradients_mean: False
  enable_alltoall: False
  full_batch: True
  search_mode: "sharding_propagation"
  strategy_ckpt_save_file: "./ckpt_strategy.ckpt"
  enable_parallel_optimizer: True

parallel_config:
  data_parallel: 1
  model_parallel: 2      # <--- 2卡模型并行
  pipeline_stage: 1      # <--- 流水线为1
  use_seq_parallel: False # 2卡时通常不开
  micro_batch_num: 16

# recompute config for pipeline_stage = 1
recompute_config:
  recompute: [7] # <--- 修正: 列表长度必须为1, 对应 pipeline_stage=1
  select_recompute:
    'feed_forward\.mul': [14] # <--- 修正: 列表长度从2改为1
    'feed_forward\.w1\.activation\.silu': [14] # <--- 修正: 列表长度从2改为1
    'feed_forward\.w1\.reshape': [14] # <--- 修正: 列表长度从2改为1
    'feed_forward\.w2\.reshape': [14] # <--- 修正: 列表长度从2改为1
  mp_comm_recompute: True
  recompute_slice_activation: False

# callbacks
callbacks:
  - type: MFLossMonitor
  - type: CheckpointMonitor
    prefix: "qwen2_5_7b_finetune"
    save_checkpoint_steps: 500
    keep_checkpoint_max: 1
    integrated_save: False

# mindspore context init config
context:
  mode: 0 # Graph Mode
  device_target: "Ascend"
  max_device_memory: "58GB" # 请根据您的NPU显存大小调整
  save_graphs: False
  device_id: 0
  ascend_config:
    precision_mode: "must_keep_origin_dtype"

# =========================== 关键错误修正 ===========================
# model config: MUST use Qwen2's config and arch, NOT Llama's
model:
  model_config:
    type: Qwen2Config # <--- 修正: LlamaConfig -> Qwen2Config
    # 以下是Qwen2-7B模型的标准参数, 无需修改
    hidden_size: 3584
    num_layers: 28
    num_heads: 28
    num_key_value_heads: 4
    intermediate_size: 18944
    vocab_size: 152064
    max_position_embeddings: 32768
    rope_theta: 1000000.0
    use_flash_attention: True
    compute_dtype: "bfloat16"
    layernorm_compute_type: "float32"
    softmax_compute_type: "float32"
    rotary_dtype: "float32"
    param_init_type: "float32"
    use_past: False
    seq_length: 8192

  arch:
    type: Qwen2ForCausalLM # <--- 修正: LlamaForCausalLM -> Qwen2ForCausalLM
```


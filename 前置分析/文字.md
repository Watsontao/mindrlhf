代码修改

版本为：v0.5.0-736-gbf9b7e41







http://pip.modelarts.private.com:8888/repository/pypi/packages/ray/2.51.1/ray-2.51.1-cp310-cp310-manylinux2014_aarch64.whl









rollouter把GRPO的8次response分开了，也就是说，trainer希望要的数据是一个rollout_sample，里面有的是1个prompt+8个response，但是现在我们把这8次response分开了，就导致了，我们的prob推理完了之后，放入到messagequeue里面的一个rollouot_sample只有1个prompt+1个response。



**问题1**

Trainer 的期望：RolloutSample = 1 Prompt + 8 Responses。它认为这是不可分割的原子单位。

P-DSR 的现状：我们把这个原子单位劈开了。

Queue 里出现了 RolloutSample (1 Prompt + 1 Response)。

Queue 里又出现了 RolloutSample (1 Prompt + 7 Responses)。

冲突：Trainer 拿到那个 "1+1" 的包时，以为它是完整的，拿去算 GRPO，结果发现“咦，怎么只有 1 个 Response？”，然后整个逻辑链条就断了（或者算出了错误的 Advantage）。

**解决1**

Rollouter 跑完 Probe，不发走，先拿着。

根据 Probe 结果，去跑 Rest。

等 Rest 跑完了，在 Rollouter 内存里把 Probe 和 Rest 合并 (Merge) 成一个完整的 Sample。

把这个完整的 Sample 发给 MessageQueue。

---



整体思路

因为GRPO对同一个样本推理n次

前提：同一个prompt rollout的长度大致相同

对同一个batch里面的所有samples，先推理一次，计算出时间/token长度，然后依据时间/token对samples进行排序，划分出长尾sample和普通sample。

但现在有个问题，同一个prompt 推理n次得到的responses，他们的token长度方差挺大的。



首先，所有 Worker 并行对 Batch 内样本进行单次探路 (Probe)；随后，依据探路长度排序，将 Top 20%的显性长任务直接转移 (Offload) 至 Heavy Worker；剩余 80% 的任务留在 Fast Worker 原地执行，但需设置 动态熔断阈值（Probe 长度的 1.5 倍）；若极少数任务（约13%）在执行中突破该阈值，则触发熔断并回流至 Heavy Worker 重跑。此策略以微小的重试代价（~5% 算力损耗），换取了 Fast Worker 显存压力的彻底释放，使其支持 2倍 Batch Size并发，从而预期实现 20% 以上 的整体时间节省。



```
python3 examples/data_preprocess/math_dataset.py --local_dir "data/math" --data_path "DigitalLearningGmbH/MATH-lighteval"
```







# 思路演进

 P-DSR 方案演进与验证路径 (论文思路梳理)

    1. 问题发现 (Problem Identification)

   * 背景：在 GRPO 强化学习训练中，推理（Rollout）阶段占据了大量时间。
   * 现象：推理过程中存在显著的 "Bubble" (空转) 现象。
   * 根因假设：推测是由于 Generation Length 的长尾分布 导致的。即：同一个 Batch 中，少数几个超长回答（Long-tail）拖慢了整体进度，且占用了大量显存，导致 Batch Size 无法提升。

    2. 初始方案 V1 (Batch-Sync Probe)

   * 思路：既然长短不一，那就先跑一次（Probe），预测出哪些是长任务，把它们分流走。
   * 假设：同一个 Prompt 的 $N$ 次回答，长度应该是 "大差不差" (Consistent) 的。

    3. 数据验证与假设推翻 (Empirical Analysis)
       为了验证 V1，我们在 0.5B 模型 + MATH 数据集上进行了实验数据采集（Log Collection）。

   * 发现 1：高方差 (High Variance)
     * 通过 analyze_variance 发现，CV (变异系数) 平均值达 0.29，最大达 1.18。说明模型输出极其不稳定。
   * 发现 2：预测失效 (Prediction Failure)
     * False Negative (漏斗率) 极高：Probe < 512 的样本中，有 63% 的概率后续会爆出 > 512 的长尾。
     * 结论：静态预测不可靠。如果按 V1 执行，Fast Worker 会被“隐形刺客”击穿，导致 OOM 或阻塞。

    4. 方案迭代 V2 (Circuit Breaker)

   * 思路调整：既然无法完美预测，就必须接受错误。引入 "软熔断 (Soft Circuit Breaker)" 机制。
   * 策略：允许分流错误，但在 Fast Worker 上设置 max_tokens 锁。一旦触网，立刻截断并重试（Retry）。
   * 新问题：如果阈值设死（如 512），重试率太高（63%），导致算力浪费严重，可能得不偿失。

    5. 最终方案 V2.5 (Dynamic Rank-Based Offloading)

   * 思路升级：从“绝对阈值”转向 "相对排名 (Percentile)"，以适应不同 Batch 的难度波动。
   * 策略：
     1. Probe & Sort：按长度排序。
     2. Top 20% Offload：移走最长的 20%（数据证明 Top 端的预测准确率高达 97%）。
     3. Dynamic Cap：利用 Top 20% 的分界线长度 $L_{cut}$，动态设定 Fast Worker 的熔断线为 $1.5 \times L_{cut}$。
   * 仿真验证 (Simulation)：
     * 利用真实 Log 数据回测该策略。
     * 结果：Failure Rate (重试率) 骤降至 13%。
     * 意义：证明了该方案在保证系统稳定性的前提下，将额外开销控制在了极低水平（<10% 算力损耗），从而为通过增大 Batch Size 换取 20% 时间提升提供了坚实的理论基础。

---

  论文写作建议：
  你可以按照 "Motivation (长尾导致效率低) -> Observation (数据揭示高方差与预测难点) -> Method (提出动态排名+熔断机制) -> Evaluation (仿真证明低重试率与高收益)"
  的逻辑线来组织内容。这样显得非常严谨且由于数据支撑而具有说服力。





##  1. Rollouter 初始化流程全解

  1. 资源规划阶段 (Driver 端)
    create_resource_pool_manager 函数根据配置生成并返回一个 ResourcePoolManager 对象。该对象核心包含两个映射表：
   * 资源映射表：{"rollout_pool": ResourcePool实例}。其中 ResourcePool实例 封装了物理资源信息（如 GPU 数量，基于脚本配置 NGPUS=1，此处即代表 1 个 GPU）。
   * 角色映射表：{Role.Rollout: "rollout_pool"}。这定义了逻辑角色 Role.Rollout 必须运行在名为 "rollout_pool" 的物理资源池上。

  2. 资源绑定阶段 (Rollouter 端)
    进入 init_workers 后，首先执行 _init_resource_pools。
   * 它从 ResourcePoolManager 中提取出 ResourcePool实例。
   * 以此实例为 Key，初始化字典 self.resource_pool_to_cls，此时其 Value 为空字典 {}，完成物理资源的“占位”。

  3. 任务分配阶段
    紧接着执行 _create_worker_classes。
   * 根据 Role.Rollout 对应的 Worker 类（DetachAsyncRolloutWorker）及其配置信息，封装成一个 RayClassWithInitArgs 启动包对象。
   * 将该对象填入 self.resource_pool_to_cls 的 Value 中，完成了“物理资源”与“执行逻辑”的绑定。

  4. 进程启动阶段
    随后执行 _init_worker_groups。
   * 它读取 self.resource_pool_to_cls 字典。
   * 根据 Key 指定的 GPU 数量和 Value 指定的 Worker 类，向 Ray 申请资源并启动真实的 Worker 进程。
   * 返回一个 RayWorkerGroup 对象（它是这组 Worker 的句柄），并将其存入 self.all_wg['rollout'] 中。

  5. 模型加载阶段
    执行 _init_models。
   * 从 self.all_wg['rollout'] 中取出 RayWorkerGroup 对象，并将其引用赋值给 self.actor_rollout_wg（方便后续调用）。
   * 通过该句柄向远程 Worker 发送指令，触发 vLLM 引擎初始化并加载模型权重到 GPU 显存。

  6. 管理层构建阶段
    最后执行 _init_async_rollout_manager。
   * 创建一个本地的 FullyAsyncAgentLoopManager 对象（即“管家”）。
   * 将 self.actor_rollout_wg（Worker 句柄）移交给这个管家，并赋值给 self.async_rollout_manager。
   * 至此初始化完成。后续所有的推理任务请求，Rollouter 只需直接通过 self.async_rollout_manager 进行调度即可。

















